{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta, Adam, rmsprop\n",
    "from time import sleep\n",
    "\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional \n",
    "\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv('../train_data_2.csv')[:300]\n",
    "Validation = pd.read_csv('../validation_data_2.csv')[:100]\n",
    "Test = pd.read_csv('../test_data_2.csv')[:100]\n",
    "\n",
    "realTweet = []\n",
    "for tweet in Train['tweet']:\n",
    "    exec('a=' + tweet)\n",
    "    realTweet.append(a)\n",
    "Train['tweet'] = realTweet\n",
    "\n",
    "realTrainTweet = []\n",
    "for tweet in Validation['tweet']:\n",
    "    exec('a=' + tweet)\n",
    "    realTrainTweet.append(a)\n",
    "Validation['tweet'] = realTrainTweet\n",
    "\n",
    "realTrainTweet = []\n",
    "for tweet in Test['tweet']:\n",
    "    exec('a=' + tweet)\n",
    "    realTrainTweet.append(a)\n",
    "Test['tweet'] = realTrainTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_word_key(string):\n",
    "    upper_part = (re.findall('_.*',string))[0].upper()\n",
    "    result = re.sub(r'_.*', upper_part, string)\n",
    "    #print(f'was {string} -> became {result}')\n",
    "    return result\n",
    "\n",
    "def DataTransformForLSTM(Train, Validation, Test):\n",
    "    maxima = 0\n",
    "    for ar in Train['tweet'].to_list():\n",
    "        if len(ar)>maxima: maxima = len(ar)\n",
    "    for ar in Validation['tweet']:\n",
    "        if len(ar)>maxima: maxima = len(ar)\n",
    "    for ar in Test['tweet']:\n",
    "        if len(ar)>maxima: maxima = len(ar)\n",
    "    max_len = maxima + 1\n",
    "    print(f'max_len = {max_len}')\n",
    "\n",
    "    tok = Tokenizer()\n",
    "    tok.fit_on_texts(Test[\"tweet\"]) #fit on train\n",
    "\n",
    "    # exctrax features from train\n",
    "    sequences_train = tok.texts_to_sequences(Train[\"tweet\"])\n",
    "    sequences_matrix_train = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "\n",
    "    # exctrax features from validation\n",
    "    sequences_val = tok.texts_to_sequences(Validation[\"tweet\"])\n",
    "    sequences_matrix_val = sequence.pad_sequences(sequences_val, maxlen=max_len)\n",
    "\n",
    "    # exctrax features from test\n",
    "    sequences_test = tok.texts_to_sequences(Test[\"tweet\"])\n",
    "    sequences_matrix_test = sequence.pad_sequences(sequences_test, maxlen=max_len)\n",
    "\n",
    "    word_index = tok.word_index\n",
    "    #print('Found %s unique tokens' % len(word_index))\n",
    "    max_words = len(word_index)\n",
    "\n",
    "    wi1 = len(word_index)\n",
    "\n",
    "    # fix tokenizer problem\n",
    "    for key in word_index.keys():\n",
    "        fixed_key = fix_word_key(key)\n",
    "        word_index[fixed_key] = word_index.pop(key)\n",
    "\n",
    "    #print('Found %s unique tokens' % len(word_index))\n",
    "    wi2 = len(word_index)\n",
    "\n",
    "    if wi1 != wi2:\n",
    "        print()\n",
    "        error = 'lenght of word_index was changed!'\n",
    "        print(error.upper())\n",
    "        raise ValueError\n",
    "    else:\n",
    "        print('Everything is OK')\n",
    "        print(len(word_index))\n",
    "\n",
    "    # делаем Embedding на основе w2v модели\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(\"../../Embeddings/model.bin\", binary=True)\n",
    "\n",
    "    nb_words = min(max_words, len(word_index))+1 # проверяем где меньше, в нашем датасете или в токенайзере.\n",
    "    EMBEDDING_DIM = 300 # размерность векторов в нашей модели w2v\n",
    "\n",
    "    print(f'number of words = {nb_words}')\n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "    print('embedding_matrix shape', embedding_matrix.shape)\n",
    "\n",
    "    counter = 0\n",
    "    for word, i in word_index.items():\n",
    "        if i >= nb_words: continue\n",
    "        if word in model.vocab:\n",
    "            #print(model[word])\n",
    "            embedding_matrix[i] = model[word]\n",
    "    #     if counter > nb_words: break\n",
    "    #     counter += 1\n",
    "\n",
    "\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    #pd.DataFrame(embedding_matrix)\n",
    "    \n",
    "    X_train = sequences_matrix_train\n",
    "    y_train = to_categorical(Train['class'])\n",
    "    X_val = sequences_matrix_val\n",
    "    y_val = to_categorical(Validation['class'])\n",
    "    X_test = sequences_matrix_test\n",
    "    y_test = to_categorical(Test['class'])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_val, y_val, embedding_matrix\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_val, y_val, embedding_layer = DataTransformForLSTM(Train, Validation, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300, 34),\n",
       " (300, 2),\n",
       " (100, 34),\n",
       " (100, 2),\n",
       " (100, 34),\n",
       " (100, 2),\n",
       " 'k',\n",
       " (859, 300))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape, 'k', embedding_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "            'pooling_size1': hp.choice('pooling_size1', [3, 5]),\n",
    "            'pooling_size2': hp.choice('pooling_size2', [3, 5]),\n",
    "    \n",
    "            'dropout1': hp.uniform('dropout1', .25,.5),\n",
    "            'dropout2': hp.uniform('dropout2', .25,.5),\n",
    "            'dropout3': hp.uniform('dropout3', .25,.5),\n",
    "    \n",
    "            'dense_size': hp.choice('dense_size', [32, 64, 128]),\n",
    "            \n",
    "            'batch_size' : hp.choice('batch_size', [32, 64]),\n",
    "            'hidden_activation': hp.choice('hidden_activation', ['relu', 'sigmoid', 'tanh', 'selu']),\n",
    "            'hidden_activation2': hp.choice('hidden_activation2', ['relu', 'sigmoid', 'tanh', 'selu']),\n",
    "            'optimizer': hp.choice('optimizer',['adadelta', 'adam', 'rmsprop']),\n",
    "            'loss': hp.choice('loss', ['binary_crossentropy']), \n",
    "            'epochs' :  15,\n",
    "            'activation': 'relu',\n",
    "            'patience': 5\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMCNN_hyperopt                                      \n",
      "val_loss: 0.545985                                    \n",
      "LSTMCNN_hyperopt                                                                \n",
      "val_loss: 0.677308                                                              \n",
      "LSTMCNN_hyperopt                                                                \n",
      "val_loss: 0.568073                                                              \n",
      "LSTMCNN_hyperopt                                                                \n",
      "val_loss: 0.664880                                                              \n",
      "LSTMCNN_hyperopt                                                                \n",
      "val_loss: 0.364403                                                              \n",
      "LSTMCNN_hyperopt                                                                 \n",
      "val_loss: 0.649480                                                               \n",
      "LSTMCNN_hyperopt                                                                 \n",
      "val_loss: 1.321903                                                               \n",
      "LSTMCNN_hyperopt                                                                 \n",
      "val_loss: 0.672032                                                               \n",
      "LSTMCNN_hyperopt                                                                 \n",
      "val_loss: 0.624839                                                               \n",
      "LSTMCNN_hyperopt                                                                 \n",
      "val_loss: 0.671767                                                               \n",
      "100%|██████████| 10/10 [02:43<00:00, 16.32s/trial, best loss: 0.36440285205841066]\n",
      "\n",
      " THE best: \n",
      "('batch_size', 1)\n",
      "('dense_size', 0)\n",
      "('dropout1', 0.39461651019515437)\n",
      "('dropout2', 0.7396057604413669)\n",
      "('dropout3', 0.4408370824285548)\n",
      "('hidden_activation', 1)\n",
      "('hidden_activation2', 1)\n",
      "('loss', 0)\n",
      "('optimizer', 2)\n",
      "('pooling_size1', 0)\n",
      "('pooling_size2', 0)\n"
     ]
    }
   ],
   "source": [
    "def model(params):\n",
    "    rate_drop_lstm  = 0.15 + np.random.rand() * 0.25\n",
    "    rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "    embedding_layer_fr   = Embedding(nb_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=max_len,\n",
    "                                    trainable=True) \n",
    "\n",
    "    embedding_layer      = Embedding(nb_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=max_len,\n",
    "                                    trainable=False) \n",
    "\n",
    "    embedding_layer_char = Embedding(nb_words,\n",
    "                                    EMBEDDING_DIM,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=max_len,\n",
    "                                    trainable=True) \n",
    "\n",
    "    # CONFIGURATION OF BI-LSTM\n",
    "    lstm_layer           = Bidirectional(LSTM(64, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\n",
    "    sequence_1_input     = Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer_fr(sequence_1_input)\n",
    "    x1                   = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "    sequence_2_input     = Input(shape=(max_len,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    y1                   = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "    # CNN topology\n",
    "    sequence_3_input     = Input(shape=(max_len,), dtype='int64', name='input_CHARS')\n",
    "    z1                   = embedding_layer_char(sequence_3_input)\n",
    "\n",
    "\n",
    "    z1                    = Conv1D(filters=300, kernel_size=3, \n",
    "                                   activation=params['hidden_activation'], input_shape=(x.shape))(z1)\n",
    "    z1                    = Dropout(params['dropout1'])(z1)\n",
    "    z1                    = MaxPooling1D(pool_size=params['pooling_size1'])(z1)\n",
    "\n",
    "    z1                    = Conv1D(filters=300, kernel_size=5, \n",
    "                                   activation=params['hidden_activation'], input_shape=(x.shape))(z1)\n",
    "    z1                    = Dropout(params['dropout2'])(z1)\n",
    "    z1                    = MaxPooling1D(pool_size=params['pooling_size2'])(z1)\n",
    "\n",
    "    z1 = Flatten()(z1)\n",
    "\n",
    "\n",
    "    num_of_classes = 2\n",
    "    dropout_p = 0.5\n",
    "    fully_connected_layers = [200, 200] \n",
    "    for dense_size in fully_connected_layers:\n",
    "        z1 = Dense(dense_size, activation=params['hidden_activation'])(z1)\n",
    "        z1 = Dropout(params['dropout3'])(z1)\n",
    "\n",
    "\n",
    "\n",
    "    merged               = concatenate([x1, y1, z1])\n",
    "    merged               = Dropout(rate_drop_dense)(merged)\n",
    "    merged               = BatchNormalization()(merged)\n",
    "    merged               = Dense(32, activation=params['hidden_activation2'])(merged)\n",
    "    merged               = Dropout(rate_drop_dense)(merged)\n",
    "    merged               = BatchNormalization()(merged)\n",
    "    preds                = Dense(2, activation='softmax')(merged)\n",
    "\n",
    "    STAMP = 'lstm_%d_%d_%.2f_%.2f'%(10, 10, rate_drop_lstm, \\\n",
    "            rate_drop_dense)\n",
    "    STAMP = 'LSTMCNN_hyperopt'\n",
    "\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input, sequence_3_input], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=params['optimizer'], \n",
    "                  metrics=['acc'])\n",
    "    #model.summary()\n",
    "    print(STAMP, end='\\n\\n')\n",
    "\n",
    "    epochs = params['epochs']\n",
    "    batch_size = params['batch_size']\n",
    "    patience = params['patience']\n",
    "\n",
    "    # form stucture of input data for cases train, val, test\n",
    "    train_data = [X_train, X_train, X_train]\n",
    "    val_data   = [X_val, X_val, X_val]\n",
    "    test_data  = [X_test, X_test, X_test]\n",
    "\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    bst_model_path = STAMP + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    history = model.fit(train_data, y_train, \n",
    "                     validation_data=(val_data, y_val), \n",
    "                     epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "                     verbose = 0,\n",
    "                     callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "\n",
    "    check = np.amax(history.history['val_loss'])\n",
    "    print(f'val_loss: {check:.6f}')\n",
    "    sys.stdout.flush() \n",
    "    return {'loss': check, 'status': STATUS_OK}\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(model, space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "print('\\n THE best: ')\n",
    "\n",
    "for parameter in best: print((parameter, best[parameter]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'space' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe093b8f519c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pooling_size1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'space' is not defined"
     ]
    }
   ],
   "source": [
    "print(space_eval(space, best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-95e16d43182d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_data)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "pred = [round(predict[0], 0) for predict in pred]\n",
    "real = [t[0] for t in y_test ]\n",
    "print(classification_report(real, pred))\n",
    "pd.DataFrame(confusion_matrix(real, bin_pred), columns=['Pos', 'Neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
