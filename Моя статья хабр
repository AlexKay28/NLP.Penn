<i>Доброго времени суток!
</i>

Библиотека FastText, в первую очередь, была разработа командой Facebook для классификации текстов, но так же может быть использована для обучения эмбедингов слов. С того момента когда FastText стал продуктом доступным для всех 2016 он получил широкое применение по причине хорошей скорости тренировки и отличной работоспособности.
<cut/>
Читая официальную документацию (очень скудная на объяснения), я понял что там содержится достаточно большая часть алгоритмов которые могут оказаться не совсем прозрачными, поэтому было решено самостоятельно разобраться как все это работает и с чем это едят. Начал я с чтения главной статьи от создателей и быстрого просмотра на Стенфордского курса по глубокому обучению в НЛП. В процессе всего этого удовольствия, вопросов у меня только прибавилось. К примеру: что в курсе лекций Стенфорда и в одной из частей статьи говорят об использовании N-грамм, но не говорят никаких особенностей по их использованию. Так-же, параметры, прописываемые в командной строке включают в себя некий bucket единственные комментарии по которому звучат как "число контейнеров(бакетов)". Как вычисляются эти N-граммы? Не понятно... Остается один вариант, смотреть код на <a href="https://github.com/facebookresearch/fastText">github</a>

<h4>После просмотра всего добра было выяснено следующее:</h4>
<ul>
<li> FastText с уверенностью использует N-граммы символов с тем же успехом как и N-граммы слов.</li>
<li>FastText поддерживает многоклассовую классификацию, что может оказаться не так уж и очевидно с первого раза</li>
<li>Число параметров модели</li>
</ul>

<h4>Введение в модель</h4>
<img src="https://miro.medium.com/max/1278/1*AgrrRZ9DpUVb3srTWs0gzA.png" alt="image"/>
Согласно статье написанной разработчиками, модель представляет из себя простейшую нейронную сеть с одним скрытым слоем. Текст представленный мешком слов (BOW) пропускается через первый слой в котором он трансформируется в эмбединги для каждого слова. В последствии полученные эмбединги осредняются по всему тексту и сводятся к одному единственному который применим по всему тексту. В скрытом слое работаем с <i>n_words * dim</i> числом параметров, где <i>dim</i> это размер эмбеддинга, а <i>n_words</i> размер используемого словаря для текста. После осреднения получаем один единственный вектор который пропускается через довольно популярный классификатор: применяется функция softmax для линого отображения(преобразования) входных данных первого слоя на последний. В качестве линейного преобразования выступает матрица размерностью <i>dim * n_output</i>, где <i>n_output</i> является нашим числом действительных классов. В оригинальной статье конечная вероятность описывается <i>log-likelyhood</i>:

<img src="https://miro.medium.com/max/218/1*gBf6hJWibfiwHDdUbiQusw.png" alt="image"/>

где:
<ul>
<li>x_n  это представление слова в n-грамме. </li>
<li>А это look_up матрица извлекающая эмбеддинг слова.</li>
<li>В это линейное преобразование выходных данных.</li>
<li>f непосредственно сама фунция softmax.</li>
</ul>

Все в целом не так плохо, но все же давайте взглянем на код:

<h4>Код как трансляция идеи</h4>
Файл с исходными данными которые мы сможем применить для нашего классификатора должны следовать конкретной форме: __label__0 (текст).

В качестве примеров:
__label__cat This thext is about cats.
__label__dog This text is about dogs. 

Исходный файл применяется как аргумент функции train. Функция train стартует с инициализации и послеюущего заполнения переменной <i>dict_</i>.

<source lang="cpp">void FastText::train(const Args args) {
  args_ = std::make_shared<Args>(args);
  dict_ = std::make_shared<Dictionary>(args_);
  if (args_->input == "-") {
    // manage expectations
    throw std::invalid_argument("Cannot use stdin for training!");
  }
  std::ifstream ifs(args_->input);
  if (!ifs.is_open()) {
    throw std::invalid_argument(
        args_->input + " cannot be opened for training!");
  }
  dict_->readFromFile(ifs);</source>

dict_ является экземпляром класса Dictionary.

<source lang="cpp">
Dictionary::Dictionary(std::shared_ptr<Args> args) : args_(args),
  word2int_(MAX_VOCAB_SIZE, -1), size_(0), nwords_(0), nlabels_(0),
  ntokens_(0), pruneidx_size_(-1) {}</source>

После считывания и парсинга предложений исходного файла, мы напоняем два вектора:
<ul>
<li> words_ , который содержит уникальные слова извлеченные из текста</li>
<li>word2int_ , который хэши для каждого слова в соответсвтии с его позицией в векторе <i>words_</i>. Это на самом деле очень важно, так как это определяет индексты которые будут использованы для поиска эмбеддингов матрицы А</li>
</ul>
Вектор <i>words_</i> содержит в себе экземпляры entry. Каждый из которых может быть типом word для <i>label</i> и иметь счетчик обращений к ним. Так же тут еще присутсвует  поле <i>subwords</i>, но его мы рассмотрим чуть дальше.
<source lang="cpp">struct entry {
  std::string word;
  int64_t count;
  entry_type type;
  std::vector<int32_t> subwords;
};</source>

Добавляя слова или метки в <i>word2int_</i> переменную, коллизии обязательно разрешаются таким образом, что мы никогда не обратимся к двум разным словам имеющим одинаковый индекс. Таких попросту не будет. 
<source lang="cpp">int32_t Dictionary::find(const std::string& w, uint32_t h) const {
  int32_t word2intsize = word2int_.size();
  int32_t id = h % word2intsize;
  while (word2int_[id] != -1 && words_[word2int_[id]].word != w) {
    id = (id + 1) % word2intsize;
  }
  return id;
}

int32_t Dictionary::find(const std::string& w) const {
  return find(w, hash(w));
}

void Dictionary::add(const std::string& w) {
  int32_t h = find(w);
  ntokens_++;
  if (word2int_[h] == -1) {
    entry e;
    e.word = w;
    e.count = 1;
    e.type = getType(w);
    words_.push_back(e);
    word2int_[h] = size_++;
  } else {
    words_[word2int_[h]].count++;
  }
}</source>

Оба вектора фильтруются, дабы убедиться в том что слова и метки, которые упоминаются хотя бы раз, включены. После переходим к части где используем функцию <i>readFromFile</i> где вызывается <i>initNgrams</i>. Мы почти подобрались к мистике использования N-gramm. 
<source lang="cpp">
void Dictionary::readFromFile(std::istream& in) {
  std::string word;
  int64_t minThreshold = 1;
  while (readWord(in, word)) {
    add(word);
    if (ntokens_ % 1000000 == 0 && args_->verbose > 1) {
      std::cerr << "\rRead " << ntokens_  / 1000000 << "M words" << std::flush;
    }
    if (size_ > 0.75 * MAX_VOCAB_SIZE) {
      minThreshold++;
      threshold(minThreshold, minThreshold);
    }
  }
  threshold(args_->minCount, args_->minCountLabel);
  initTableDiscard();
  initNgrams();</source>

Функция <i>initNgrams</i> определяет все комбинации N-грамм символов и добавляет их в вектор <i>ngram</i>. В конце концов, обсчитываются все хеши для N-грамм и складываются в векторе <i>ngram</i>, формируя размер <i>bucket</i>. Другими словами, хэши N-грамм символов добавляются после добавления хэшей N-грамм слов. Как следствие их индексы не пересекаются с индексами слов, но могут пересекаться друг с другом. 

В целом, для каждого слова в тексте можно предоставить <i>subwords</i>... N-граммы символов.
Матрица эмбеддингов А отображает следующее:
<ul>
	<li>Изначальные nwords_ строк содержащие эмбеддинги для каждого слова из имеющегося для текста словаря.</li>
<li>Последующте bucket строк содержащие в себе эмбеддинги для каждой N-граммы символов</li>
</ul> 
<source lang="cpp">
void Dictionary::initNgrams() {
  for (size_t i = 0; i < size_; i++) {
    std::string word = BOW + words_[i].word + EOW;
    words_[i].subwords.clear();
    words_[i].subwords.push_back(i);
    if (words_[i].word != EOS) {
      computeSubwords(word, words_[i].subwords);
    }
  }
}

void Dictionary::computeSubwords(const std::string& word,
                               std::vector<int32_t>& ngrams) const {
  for (size_t i = 0; i < word.size(); i++) {
    std::string ngram;
    if ((word[i] & 0xC0) == 0x80) continue;
    for (size_t j = i, n = 1; j < word.size() && n <= args_->maxn; n++) {
      ngram.push_back(word[j++]);
      while (j < word.size() && (word[j] & 0xC0) == 0x80) {
        ngram.push_back(word[j++]);
      }
      if (n >= args_->minn && !(n == 1 && (i == 0 || j == word.size()))) {
        int32_t h = hash(ngram) % args_->bucket;
        pushHash(ngrams, h);
      }
    }
  }
}

void Dictionary::pushHash(std::vector<int32_t>& hashes, int32_t id) const {
  if (pruneidx_size_ == 0 || id < 0) return;
  if (pruneidx_size_ > 0) {
    if (pruneidx_.count(id)) {
      id = pruneidx_.at(id);
    } else {
      return;
    }
  }
  hashes.push_back(nwords_ + id);
}</source>

Т.о мы разгадали мистичные N-грамм символов. Давайте же разберемся теперь с N-граммами слов.

Возвращаясь к функции <i>train</i>, выполняются следующие инструкции:
<source lang="cpp">  if (args_->pretrainedVectors.size() != 0) {
    loadVectors(args_->pretrainedVectors);
  } else {
    input_ = std::make_shared<Matrix>(dict_->nwords()+args_->bucket, args_->dim);
    input_->uniform(1.0 / args_->dim);
  }

  if (args_->model == model_name::sup) {
    output_ = std::make_shared<Matrix>(dict_->nlabels(), args_->dim);
  } else {
    output_ = std::make_shared<Matrix>(dict_->nwords(), args_->dim);
  }
  output_->zero();
  startThreads();</source>

Это то место где инициализируется матрица эмбеддингов А. Нужно указать, что если pretrainedVectors проходят работу с этой функцией, то данная переменная считается наполненой. В случае если такового не случается, то матрица инициализируется случайным числами <i>-1/dim </i> и <i>1/dim</i>, где <i>dim</i> - размер нашего эмбеддинга. Размерность матрицы А (<i>n_words_</i> + <i>bucket</i> ; <i>dim</i>), т.о. мы собираемся настраивать все эти эмбеддинги для каждого слова. Выходные данные так же инициализируются на этом шаге.

Как итог мы получаем часть где мы начинаем тренировку нашей модели. 
<source lang="cpp">
void FastText::trainThread(int32_t threadId) {
  std::ifstream ifs(args_->input);
  utils::seek(ifs, threadId * utils::size(ifs) / args_->thread);

  Model model(input_, output_, args_, threadId);
  if (args_->model == model_name::sup) {
    model.setTargetCounts(dict_->getCounts(entry_type::label));
  } else {
    model.setTargetCounts(dict_->getCounts(entry_type::word));
  }

  const int64_t ntokens = dict_->ntokens();
  int64_t localTokenCount = 0;
  std::vector<int32_t> line, labels;
  while (tokenCount_ < args_->epoch * ntokens) {
    real progress = real(tokenCount_) / (args_->epoch * ntokens);
    real lr = args_->lr * (1.0 - progress);
    if (args_->model == model_name::sup) {
      localTokenCount += dict_->getLine(ifs, line, labels);
      supervised(model, lr, line, labels);
    } else if (args_->model == model_name::cbow) {
      localTokenCount += dict_->getLine(ifs, line, model.rng);
      cbow(model, lr, line);
    } else if (args_->model == model_name::sg) {
      localTokenCount += dict_->getLine(ifs, line, model.rng);
      skipgram(model, lr, line);
    }
    if (localTokenCount > args_->lrUpdateRate) {
      tokenCount_ += localTokenCount;
      localTokenCount = 0;
      if (threadId == 0 && args_->verbose > 1)
        loss_ = model.getLoss();
    }
  }
  if (threadId == 0)
    loss_ = model.getLoss();
  ifs.close();
}</source>

В данной части когда происходит два глвных момента. Первый, функция <i>getLine</i> вызывает перменную <i>dict_</i>. Воторой, вызывается функция <i>supervised</i>.
<source lang="cpp">int32_t Dictionary::getLine(std::istream& in,
                            std::vector<int32_t>& words,
                            std::vector<int32_t>& labels) const {
  std::vector<int32_t> word_hashes;
  std::string token;
  int32_t ntokens = 0;

  reset(in);
  words.clear();
  labels.clear();
  while (readWord(in, token)) {
    uint32_t h = hash(token);
    int32_t wid = getId(token, h);
    entry_type type = wid < 0 ? getType(token) : getType(wid);

    ntokens++;
    if (type == entry_type::word) {
      addSubwords(words, token, wid);
      word_hashes.push_back(h);
    } else if (type == entry_type::label && wid >= 0) {
      labels.push_back(wid - nwords_);
    }<source lang="cpp">
    if (token == EOS) break;
  }
  addWordNgrams(words, word_hashes, args_->wordNgrams);
  return ntokens;
}</source>

В функции выше, мы читаем тексты из входных данных, определяем индексы для каждого слова друг за другом посредством использования <i>word2int_</i>. Мы добавляем составляющие эти слова N-граммы в переменную <i>words</i>, как указано в коде. И в конце концов мы добавляем метки прямиком в вектор <i>labels</i>. 

После полноценного считывания и добавления текста(предложения) прямиком в созданные для них вектора, мы получаем часть кода которая обрабатывает N-граммы. Это функция <i>addWordNgrams</i>.
<source lang="cpp">void Dictionary::addWordNgrams(std::vector<int32_t>& line,
                               const std::vector<int32_t>& hashes,
                               int32_t n) const {
  for (int32_t i = 0; i < hashes.size(); i++) {
    uint64_t h = hashes[i];
    for (int32_t j = i + 1; j < hashes.size() && j < i + n; j++) {
      h = h * 116049371 + hashes[j];
      pushHash(line, h % args_->bucket);
    }
  }
}</source>

Смотрим дальше. Переменная <i>hashes</i> представляет собой набор хэшей для каждого слова текста, где <i>line</i> содержит номера слов в предложении и числа используемых N-грамм. Параметр <i>n</i> является параметром <i>wordNgrams</i> и указывает максимальную слину N-граммы слов. Каждая N-грамма слов получает свой собственный хэш высчитанные рекурсивной формулой $$display$$h = h*116049371+hashes[j]$$display$$. Данная формула представялет собой <a href="https://en.wikipedia.org/wiki/Fowler%E2%80%93Noll%E2%80%93Vo_hash_function">FNV алгоритм</a> хеширования применяемого к строке: он берет хэш каждого слова в N-грамме слов и складывает их. Таким образом получается набор уникальных хэшей. В итоге, это значение (может оказаться довольно большим) передается по модулю.

Таким образом, N-граммы слов вычисляются приблизительно так же как и N-граммы символов, но с небольшим отличием, т.к мы не хешируем конкретное слово. Удивительный ход.

После считывания предложения, вызывается функция <i>supervised</i>. В случае если предложение эмеет несколько меток, мы случайно выбираем одну из них.
  <source lang="cpp">
void FastText::supervised(
    Model& model,
    real lr,
    const std::vector<int32_t>& line,
    const std::vector<int32_t>& labels) {
  if (labels.size() == 0 || line.size() == 0) return;
  std::uniform_int_distribution<> uniform(0, labels.size() - 1);
  int32_t i = uniform(model.rng);
  model.update(line, labels[i], lr);
}</source>

И на конец, функция обновления модели.
<source lang="cpp">void Model::computeHidden(const std::vector<int32_t>& input, Vector& hidden) const {
  assert(hidden.size() == hsz_);
  hidden.zero();
  for (auto it = input.cbegin(); it != input.cend(); ++it) {
    if(quant_) {
      hidden.addRow(*qwi_, *it);
    } else {
      hidden.addRow(*wi_, *it);
    }
  }
  hidden.mul(1.0 / input.size());
}

void Model::update(const std::vector<int32_t>& input, int32_t target, real lr) {
  assert(target >= 0);
  assert(target < osz_);
  if (input.size() == 0) return;
  computeHidden(input, hidden_);
  if (args_->loss == loss_name::ns) {
    loss_ += negativeSampling(target, lr);
  } else if (args_->loss == loss_name::hs) {
    loss_ += hierarchicalSoftmax(target, lr);
  } else {
    loss_ += softmax(target, lr);
  }
  nexamples_ += 1;

  if (args_->model == model_name::sup) {
    grad_.mul(1.0 / input.size());
  }
  for (auto it = input.cbegin(); it != input.cend(); ++it) {
    wi_->addRow(grad_, *it, 1.0);
  }
}</source>

Входные переменные проходящие через функцию <i>supervised</i> имеют список индексов всех своих составляющих (слов, N-грамм слов и символов) предложения. Целью является определение класса на выходе. Функция <i>computeHidden</i> определяет все эмбеддинги для каждого составляющего входных данных посредством поиска их в матрице <i>wi_</i> и осредняя (суммируется <i>addRow</i> и делится на их размер). После модификации вектора <i>hidden_</i> отправляем их на активацию в softmax функцию и определяем метку.

Этот участок кода показывает применение функции активации <i>softmax</i>. Так же вычисляется<i> log-loss</i>.
<source lang="cpp">void Model::computeOutputSoftmax(Vector& hidden, Vector& output) const {
  if (quant_ && args_->qout) {
    output.mul(*qwo_, hidden);
  } else {
    output.mul(*wo_, hidden);
  }
  real max = output[0], z = 0.0;
  for (int32_t i = 0; i < osz_; i++) {
    max = std::max(output[i], max);
  }
  for (int32_t i = 0; i < osz_; i++) {
    output[i] = exp(output[i] - max);
    z += output[i];
  }
  for (int32_t i = 0; i < osz_; i++) {
    output[i] /= z;
  }
}

void Model::computeOutputSoftmax() {
  computeOutputSoftmax(hidden_, output_);
}

real Model::softmax(int32_t target, real lr) {
  grad_.zero();
  computeOutputSoftmax();
  for (int32_t i = 0; i < osz_; i++) {
    real label = (i == target) ? 1.0 : 0.0;
    real alpha = lr * (label - output_[i]);
    grad_.addRow(*wo_, i, alpha);
    wo_->addRow(hidden_, i, alpha);
  }
  return -log(output_[target]);
}</source>

Используя данный способ мы не получаем роста размера N-грамм слов и символов. Рост блокируется имеющимся числом <i>buckets</i>. 

По умолчанию FastText не использует N-граммы, но нам рекомендуются следующие параметры:
<ul>
<li>bucker = 2000000; wordNgrams > 1 или maxn > 0 </li>
<li>dim=100</li>
<li>n_output=2</li>
<li>n_words=500000</lI>
</ul>

В сумме поулчаем довольно большое число параметров для обучения $$display$$(5000000+2000000)*100+(2*100) = 250,000,000$$display$$. Многовато, не так ли?) Как мы видим даже через такой простой подход мы имеем довольно большое число параметров, что очень даже типично для методов глубокого обучения. Используются очень грубая оценка, как например, число N-грамм взятое за 2_000_000, для того что бы показать порядок. В целом, сложность модели можно понизить путем настройки некоторых гипермараметров, таких как <i>bucket</i> или же порог взятия выборки.

Любительский перевод оригинальной статьи: <a href="https://medium.com/@mariamestre/fasttext-stepping-through-the-code-259996d6ebc4">FastText: stepping through the code</a>
<i>Небольшое предупреждение: часть представленной информации может оказаться не полностью верной в силу течения времени и случайных ошибок автора. В любом случае, любой фидбек будет желательным!</i>


Некоторые ссылки могут оказаться полезными:
https://research.fb.com/fasttext/
https://arxiv.org/pdf/1607.01759.pdf and https://arxiv.org/pdf/1607.04606v1.pdf
https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLU40WL8Ol94IJzQtileLTqGZuXtGlLMP_ (from 47:39)
