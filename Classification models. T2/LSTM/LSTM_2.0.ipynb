{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional, TimeDistributed, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=6047, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = FT_gensim(size=100)\n",
    "corpus_file = '../task2_data/task2_en_training.tsv'\n",
    "\n",
    "model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "model.train(\n",
    "    corpus_file=corpus_file, \n",
    "    epochs=model.epochs,\n",
    "    total_examples=model.corpus_count, \n",
    "    total_words=model.corpus_total_words, \n",
    "    window = 3)\n",
    "\n",
    "print(model)\n",
    "\n",
    "df = pd.read_csv('../task2_data/task2_en_training.tsv', sep='\\t')[['class', 'tweet']]\n",
    "\n",
    "df_ft = df.iloc[np.random.permutation(len(df))]\n",
    "df_ft['class'] = df_ft['class'].apply(lambda x: int(x))\n",
    "df_train, df_test = train_test_split(df_ft , test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/home/alexkay/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16435,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split into training and test data.\n",
    "X_train = df_train['tweet']\n",
    "X_test = df_test['tweet']\n",
    "y_train = df_train['class']\n",
    "y_test = df_test['class']\n",
    "\n",
    "# it's special. we can't work with other format\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "#по сути на этом этапе можно слегка почистить составляющие твитов\n",
    "for i in range(len(X_train)):\n",
    "    sentence = X_train[i].split(' ')\n",
    "    sentence = list(map(lambda x: model[x], sentence))\n",
    "    X_train[i] = sentence\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    sentence = X_test[i].split(' ')\n",
    "    sentence = list(map(lambda x: model[x], sentence))\n",
    "    X_test[i] = sentence\n",
    "\n",
    "maxima = 0\n",
    "for ar in X_train:\n",
    "    if len(ar)>maxima: maxima = len(ar)\n",
    "for ar in X_test:\n",
    "    if len(ar)>maxima: maxima = len(ar)\n",
    "max_len = maxima + 1\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pads(ars, max_len):\n",
    "    for i in range(len(ars)):\n",
    "        while len(ars[i]) < max_len: ars[i].insert(0,np.array([0 for i in range(100)]))\n",
    "    return ars\n",
    "X_train = add_pads(X_train, max_len)\n",
    "X_test = add_pads(X_test, max_len)\n",
    "X_train = X_train.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train).reshape(len(X_train), len(X_train[1]), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (5, 10)                   4440      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (5, 2)                    22        \n",
      "=================================================================\n",
      "Total params: 4,462\n",
      "Trainable params: 4,462\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "16435/16435 [==============================] - 30s 2ms/step - loss: 0.2933 - acc: 0.9064\n",
      "Epoch 2/5\n",
      "16435/16435 [==============================] - 30s 2ms/step - loss: 0.2798 - acc: 0.9066\n",
      "Epoch 3/5\n",
      "16435/16435 [==============================] - 30s 2ms/step - loss: 0.2752 - acc: 0.9067\n",
      "Epoch 4/5\n",
      "16435/16435 [==============================] - 30s 2ms/step - loss: 0.2713 - acc: 0.9063\n",
      "Epoch 5/5\n",
      "16435/16435 [==============================] - 30s 2ms/step - loss: 0.2705 - acc: 0.9071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f592f9e6d90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure network\n",
    "#n_batch = len(X) # !!!!!\n",
    "n_batch = 5\n",
    "n_epoch = 5\n",
    "n_neurons = 10\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X_train.shape[1], X_train.shape[2]), stateful=True))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['acc'])\n",
    "print(model.summary())\n",
    "\n",
    "# fit network\n",
    "model.fit(X_train, to_categorical(y_train, num_classes=2, dtype='float32'), epochs=n_epoch, batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4109, 63, 100), (16435, 63, 100))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.tolist()\n",
    "X_test = np.array(X_test).reshape(len(X_test), len(X_test[1]), 100)\n",
    "X_test.shape, X_train.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 1\n",
    "# re-define model\n",
    "new_model = Sequential()\n",
    "new_model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X_test.shape[1], X_test.shape[2]), stateful=True))\n",
    "new_model.add(Dense(2, activation='softmax'))\n",
    "# copy weights\n",
    "old_weights = model.get_weights()\n",
    "new_model.set_weights(old_weights)\n",
    "# compile model\n",
    "new_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = new_model.evaluate(X_test, \n",
    "                            to_categorical(y_test, num_classes=2, dtype='float32'), \n",
    "                            batch_size=1, \n",
    "                            verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 90.44%\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
