{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344266386467606528</td>\n",
       "      <td>809439366</td>\n",
       "      <td>0</td>\n",
       "      <td>depression hurts, cymbalta can help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>349220537903489025</td>\n",
       "      <td>323112996</td>\n",
       "      <td>0</td>\n",
       "      <td>@jessicama20045 right, but cipro can make thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>351421773079781378</td>\n",
       "      <td>713100330</td>\n",
       "      <td>0</td>\n",
       "      <td>@fibby1123 are you on paxil .. i need help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326594278472171520</td>\n",
       "      <td>543113070</td>\n",
       "      <td>0</td>\n",
       "      <td>@redicine the lamotrigine and sjs just made ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345567138376994816</td>\n",
       "      <td>138795534</td>\n",
       "      <td>0</td>\n",
       "      <td>have decided to skip my #humira shot today. my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20539</th>\n",
       "      <td>469009954251481088</td>\n",
       "      <td>23177032</td>\n",
       "      <td>1</td>\n",
       "      <td>@hornetweb my mri scan shows when it happens b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20540</th>\n",
       "      <td>468518427125358592</td>\n",
       "      <td>1961096371</td>\n",
       "      <td>1</td>\n",
       "      <td>remember 2003. vioxx costs $65m to ontario dru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20541</th>\n",
       "      <td>512254891361075200</td>\n",
       "      <td>321391071</td>\n",
       "      <td>1</td>\n",
       "      <td>asians are at higher risk for severe allergic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20542</th>\n",
       "      <td>491775200610893825</td>\n",
       "      <td>2484689840</td>\n",
       "      <td>1</td>\n",
       "      <td>5. so what caused the #estrogen surges in #nuv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20543</th>\n",
       "      <td>535492308817682432</td>\n",
       "      <td>330650218</td>\n",
       "      <td>1</td>\n",
       "      <td>@twittalesskels ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ i'm high off this tamiflu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20544 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id     user_id  class  \\\n",
       "0      344266386467606528   809439366      0   \n",
       "1      349220537903489025   323112996      0   \n",
       "2      351421773079781378   713100330      0   \n",
       "3      326594278472171520   543113070      0   \n",
       "4      345567138376994816   138795534      0   \n",
       "...                   ...         ...    ...   \n",
       "20539  469009954251481088    23177032      1   \n",
       "20540  468518427125358592  1961096371      1   \n",
       "20541  512254891361075200   321391071      1   \n",
       "20542  491775200610893825  2484689840      1   \n",
       "20543  535492308817682432   330650218      1   \n",
       "\n",
       "                                                   tweet  \n",
       "0                    depression hurts, cymbalta can help  \n",
       "1      @jessicama20045 right, but cipro can make thin...  \n",
       "2             @fibby1123 are you on paxil .. i need help  \n",
       "3      @redicine the lamotrigine and sjs just made ch...  \n",
       "4      have decided to skip my #humira shot today. my...  \n",
       "...                                                  ...  \n",
       "20539  @hornetweb my mri scan shows when it happens b...  \n",
       "20540  remember 2003. vioxx costs $65m to ontario dru...  \n",
       "20541  asians are at higher risk for severe allergic ...  \n",
       "20542  5. so what caused the #estrogen surges in #nuv...  \n",
       "20543  @twittalesskels ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ i'm high off this tamiflu...  \n",
       "\n",
       "[20544 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../task2_data/task2_en_training.tsv',delimiter='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'en' language\n"
     ]
    }
   ],
   "source": [
    "spacy_udpipe.download(\"en\")\n",
    "nlp = spacy_udpipe.load(\"en\")\n",
    "\n",
    "def lemmatize_with_postag(text = \"\"\"\"Wikipedia is a free online encyclopedia, created and \n",
    "                          edited by volunteers around the world.\"\"\",\n",
    "                          nlp=nlp):\n",
    "    doc = nlp(text)\n",
    "    tagged = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        if lemma[0] in '@?':\n",
    "            continue        \n",
    "        pos = token.pos_\n",
    "        #pos = pos.split('=')[0].strip()\n",
    "        if pos not in [\"PUNCT\"]: #[\"VERB\", \"NUM\", \"ADV\", \"NOUN\", \"ADJ\", \"AUX\", \"PRON\", \"ADP\", \"DET\", \"ADV\", \"INTJ\", \"PROPN\"]:\n",
    "            tagged.append(lemma.lower() + '_' + pos)\n",
    "        #else : print(pos)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = pd.concat([df['tweet'].apply(lemmatize_with_postag), df['class']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14380, 2), (3082, 2), (3082, 2))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "prepared_data = shuffle(prepared_data)\n",
    "\n",
    "train_data, test_data = train_test_split(prepared_data, test_size=0.3)\n",
    "test_data, validation_data = train_test_split(test_data, test_size=0.5)\n",
    "\n",
    "train_data.shape, validation_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(r'train_data_2.csv', index=False, header=True)\n",
    "validation_data.to_csv(r'validation_data_2.csv', index=False, header=True)\n",
    "test_data.to_csv(r'test_data_2.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 123.csv\t\t        LSTM\t\t      test_data_2.csv\r\n",
      " check_sigmoid.ipynb\t        LSTM_CNN.ipynb\t      train_data_2.csv\r\n",
      "'Classic methods '\t        myclass.py\t      Untitled.ipynb\r\n",
      " CNN\t\t\t        __pycache__\t      validation_data_2.csv\r\n",
      " Data_discovering_T2_T3.ipynb   Shuffle_split.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['some_DET', 'lady_NOUN', 'on_ADP', 'the_DET',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['like_INTJ', 'my_PRON', 'bitch_NOUN', 'ass_NO...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['xarelto_NOUN', 'have_AUX', 'have_VERB', 'the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['it_PRON', 'be_AUX', 'over_ADV', 'as_ADP', 'o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['oh_INTJ', 'my_PRON', 'god_NOUN', 'he_PRON', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3077</th>\n",
       "      <td>['60_NUM', '+_SYM', 'hour_NOUN', 'awake_ADJ', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3078</th>\n",
       "      <td>['up_ADV', 'early_ADV', 'for_ADP', 'my_PRON', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3079</th>\n",
       "      <td>['thank_VERB', 'you_PRON', 'for_ADP', 'the_DET...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080</th>\n",
       "      <td>['kutcher_DET', 'hoist_NOUN', 'a_DET', 'camera...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3081</th>\n",
       "      <td>['oh_INTJ', 'yep_INTJ', 'the_DET', 'only_ADJ',...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3082 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  class\n",
       "0     ['some_DET', 'lady_NOUN', 'on_ADP', 'the_DET',...      0\n",
       "1     ['like_INTJ', 'my_PRON', 'bitch_NOUN', 'ass_NO...      0\n",
       "2     ['xarelto_NOUN', 'have_AUX', 'have_VERB', 'the...      0\n",
       "3     ['it_PRON', 'be_AUX', 'over_ADV', 'as_ADP', 'o...      0\n",
       "4     ['oh_INTJ', 'my_PRON', 'god_NOUN', 'he_PRON', ...      0\n",
       "...                                                 ...    ...\n",
       "3077  ['60_NUM', '+_SYM', 'hour_NOUN', 'awake_ADJ', ...      0\n",
       "3078  ['up_ADV', 'early_ADV', 'for_ADP', 'my_PRON', ...      0\n",
       "3079  ['thank_VERB', 'you_PRON', 'for_ADP', 'the_DET...      0\n",
       "3080  ['kutcher_DET', 'hoist_NOUN', 'a_DET', 'camera...      0\n",
       "3081  ['oh_INTJ', 'yep_INTJ', 'the_DET', 'only_ADJ',...      1\n",
       "\n",
       "[3082 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('validation_data_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
