{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional \n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "#from pymystem3 import Mystem\n",
    "#from stanfordcorenlp import StanfordCoreNLP\n",
    "import spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344266386467606528</td>\n",
       "      <td>809439366</td>\n",
       "      <td>0</td>\n",
       "      <td>depression hurts, cymbalta can help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>349220537903489025</td>\n",
       "      <td>323112996</td>\n",
       "      <td>0</td>\n",
       "      <td>@jessicama20045 right, but cipro can make thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>351421773079781378</td>\n",
       "      <td>713100330</td>\n",
       "      <td>0</td>\n",
       "      <td>@fibby1123 are you on paxil .. i need help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326594278472171520</td>\n",
       "      <td>543113070</td>\n",
       "      <td>0</td>\n",
       "      <td>@redicine the lamotrigine and sjs just made ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345567138376994816</td>\n",
       "      <td>138795534</td>\n",
       "      <td>0</td>\n",
       "      <td>have decided to skip my #humira shot today. my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    user_id  class  \\\n",
       "0  344266386467606528  809439366      0   \n",
       "1  349220537903489025  323112996      0   \n",
       "2  351421773079781378  713100330      0   \n",
       "3  326594278472171520  543113070      0   \n",
       "4  345567138376994816  138795534      0   \n",
       "\n",
       "                                               tweet  \n",
       "0                depression hurts, cymbalta can help  \n",
       "1  @jessicama20045 right, but cipro can make thin...  \n",
       "2         @fibby1123 are you on paxil .. i need help  \n",
       "3  @redicine the lamotrigine and sjs just made ch...  \n",
       "4  have decided to skip my #humira shot today. my...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../task2_data/task2_en_training.tsv',delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'en' language\n"
     ]
    }
   ],
   "source": [
    "spacy_udpipe.download(\"en\")\n",
    "nlp = spacy_udpipe.load(\"en\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"../Embeddings/model.bin\", binary=True) \n",
    "#model.vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_postag(text = \"\"\"\"Wikipedia is a free online encyclopedia, created and \n",
    "                          edited by volunteers around the world.\"\"\",\n",
    "                          nlp=nlp):\n",
    "    doc = nlp(text)\n",
    "    tagged = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        pos = token.pos_\n",
    "        #pos = pos.split('=')[0].strip()\n",
    "        if pos not in [\"PUNCT\"]: #[\"VERB\", \"NUM\", \"ADV\", \"NOUN\", \"ADJ\", \"AUX\", \"PRON\", \"ADP\", \"DET\", \"ADV\", \"INTJ\", \"PROPN\"]:\n",
    "            tagged.append(lemma.lower() + '_' + pos)\n",
    "        #else : print(pos)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = df['tweet']\n",
    "y_train = df['class']\n",
    "\n",
    "Xy = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# paramters of the text praprations\n",
    "n_items_for_train = 50\n",
    "n_items_for_test  = 100\n",
    "max_words         = 500 \n",
    "\n",
    "# n_items_for_train твитов для тренировки с индексом класса 1\n",
    "df_1 = Xy[Xy['class'] == 1].iloc[:n_items_for_train] \n",
    "# n_items_for_train твитов для тренировки с индексом класса 0\n",
    "df_0 = Xy[Xy['class'] == 0].iloc[:n_items_for_train] \n",
    "\n",
    "Xy_ = shuffle(pd.concat([df_0, df_1]))\n",
    "Xy_train = pd.concat([Xy_[\"tweet\"].apply(lemmatize_with_postag), Xy_[\"class\"]], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# n_items_for_test твитов для теста с индексом класса 1\n",
    "df_1 = Xy[Xy['class'] == 1].iloc[n_items_for_train:n_items_for_train+n_items_for_test]\n",
    "# n_items_for_test твитов для теста с индексом класса 0\n",
    "df_0 = Xy[Xy['class'] == 0].iloc[n_items_for_train:n_items_for_train+n_items_for_test]\n",
    "\n",
    "Xy_ = shuffle(pd.concat([df_0, df_1]))\n",
    "Xy_test = pd.concat([Xy_[\"tweet\"].apply(lemmatize_with_postag), Xy_[\"class\"]], axis = 1)\n",
    "\n",
    "print(Xy_train.shape)\n",
    "print(Xy_test.shape)\n",
    "#Xy_train, Xy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>[i_PRON, love_VERB, reece_NOUN, webster_NOUN, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>[i_PRON, should_AUX, not_PART, tweet_VERB, on_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[@carfincomray_NOUN, hope_NOUN, you_PRON, be_A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>[depression_NOUN, medication_NOUN, mirtazapine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>[rt_NOUN, @thelsuzebra_NOUN, @_kassidymariee_V...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1278</th>\n",
       "      <td>[@cascgurl_VERB, if_SCONJ, they_PRON, force_VE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1619</th>\n",
       "      <td>[a_DET, big_ADJ, ol_NOUN, fuck_VERB, you_PRON,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>[extra_ADJ, quetiapine_NOUN, and_CCONJ, two_NU...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>[all_DET, i_PRON, want_VERB, be_AUX, a_DET, li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>[take_VERB, vyvanse_NOUN, in_ADP, the_DET, sum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  class\n",
       "62    [i_PRON, love_VERB, reece_NOUN, webster_NOUN, ...      0\n",
       "1159  [i_PRON, should_AUX, not_PART, tweet_VERB, on_...      1\n",
       "51    [@carfincomray_NOUN, hope_NOUN, you_PRON, be_A...      0\n",
       "115   [depression_NOUN, medication_NOUN, mirtazapine...      0\n",
       "1567  [rt_NOUN, @thelsuzebra_NOUN, @_kassidymariee_V...      1\n",
       "...                                                 ...    ...\n",
       "1278  [@cascgurl_VERB, if_SCONJ, they_PRON, force_VE...      1\n",
       "1619  [a_DET, big_ADJ, ol_NOUN, fuck_VERB, you_PRON,...      1\n",
       "1694  [extra_ADJ, quetiapine_NOUN, and_CCONJ, two_NU...      1\n",
       "137   [all_DET, i_PRON, want_VERB, be_AUX, a_DET, li...      0\n",
       "90    [take_VERB, vyvanse_NOUN, in_ADP, the_DET, sum...      0\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 33\n"
     ]
    }
   ],
   "source": [
    "# replcade as parameter in the begining\n",
    "#max_words = 2000 # ограничение вокуабуляра\n",
    "\n",
    "maxima = 0\n",
    "for ar in Xy_train['tweet'].to_list():\n",
    "    if len(ar)>maxima: maxima = len(ar)\n",
    "for ar in Xy_test['tweet']:\n",
    "    if len(ar)>maxima: maxima = len(ar)\n",
    "max_len = maxima + 1\n",
    "print(f'max_len = {max_len}')\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(Xy_train[\"tweet\"]) #fit on train\n",
    "\n",
    "# exctrax features from train\n",
    "sequences_train = tok.texts_to_sequences(Xy_train[\"tweet\"])\n",
    "sequences_matrix_train = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "\n",
    "# exctrax features from test\n",
    "sequences_test = tok.texts_to_sequences(Xy_test[\"tweet\"])\n",
    "sequences_matrix_test = sequence.pad_sequences(sequences_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 33)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is OK\n",
      "839\n"
     ]
    }
   ],
   "source": [
    "def fix_word_key(string):\n",
    "    upper_part = (re.findall('_.*',string))[0].upper()\n",
    "    result = re.sub(r'_.*', upper_part, string)\n",
    "    #print(f'was {string} -> became {result}')\n",
    "    return result\n",
    "\n",
    "word_index = tok.word_index\n",
    "#print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "wi1 = len(word_index)\n",
    "\n",
    "# fix tokenizer problem\n",
    "for key in word_index.keys():\n",
    "    fixed_key = fix_word_key(key)\n",
    "    word_index[fixed_key] = word_index.pop(key)\n",
    "    \n",
    "#print('Found %s unique tokens' % len(word_index))\n",
    "wi2 = len(word_index)\n",
    "\n",
    "if wi1 != wi2:\n",
    "    print()\n",
    "    error = 'lenght of word_index was changed!'\n",
    "    print(error.upper())\n",
    "    raise ValueError\n",
    "else:\n",
    "    print('Everything is OK')\n",
    "    print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words = 501\n",
      "(501, 300)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6a34bec952fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnb_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "# делаем Embedding на основе w2v модели\n",
    "\n",
    "nb_words = min(max_words, len(word_index))+1 # проверяем где меньше, в нашем датасете или в токенайзере.\n",
    "EMBEDDING_DIM = 300 # размерность векторов в нашей модели w2v\n",
    "\n",
    "print(f'number of words = {nb_words}')\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "counter = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words: continue\n",
    "    if word in w2v_model.vocab:\n",
    "        print(word, i)\n",
    "        embedding_matrix[i] = w2v_model[word]\n",
    "        #break\n",
    "#     if counter > nb_words: break\n",
    "#     counter += 1\n",
    "\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "#pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BI-LSTM (3.0)\n",
    "\n",
    "https://stackoverflow.com/questions/44924690/keras-the-difference-between-lstm-dropout-and-lstm-recurrent-dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_drop_lstm  = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "act = 'relu'\n",
    "\n",
    "\n",
    "embedding_layer      = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=False) \n",
    "lstm_layer           = Bidirectional(LSTM(64, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\n",
    "# CONFIGURATION OF BI-LSTM\n",
    "sequence_1_input     = Input(shape=(max_len,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1                   = lstm_layer(embedded_sequences_1)\n",
    "merged               = Dropout(rate_drop_dense)(x1)\n",
    "merged               = BatchNormalization()(merged)\n",
    "merged               = Dense(32, activation=act)(merged)\n",
    "merged               = Dropout(rate_drop_dense)(merged)\n",
    "merged               = BatchNormalization()(merged)\n",
    "preds                = Dense(1, activation='sigmoid')(merged)\n",
    "#preds                = Dense(2, activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(10, 10, rate_drop_lstm, \\\n",
    "        rate_drop_dense)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "print(STAMP, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from myclass import DataGenerator\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "patience = 20\n",
    "\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=patience)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# ================================================================================= ->\n",
    "# Parameters\n",
    "params = {'batch_size': batch_size, #(n_items_for_train, max_len, EMBEDDING_DIM), # (number of channels, number of classes, batch size)\n",
    "          'dim': max_len,\n",
    "          'n_classes': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "\n",
    "np.array([[index, seq] for index, seq in enumerate(sequences_matrix_train.tolist())])\n",
    "np.array([[index, seq] for index, seq in enumerate(sequences_matrix_test.tolist())])\n",
    "\n",
    "# IDs    {'train': ['id-1', 'id-2', 'id-3'], 'validation': ['id-4']}\n",
    "partition_train = sequences_matrix_train#.tolist()\n",
    "partition_val   = sequences_matrix_test[:n_items_for_test//2] #.tolist()[:n_items_for_test//2]\n",
    "partition_test  = sequences_matrix_test[n_items_for_test//2:] #.tolist()[n_items_for_test//2:]\n",
    "\n",
    "# Labels {'id-1': 0, 'id-2': 1, 'id-3': 2, 'id-4': 1}\n",
    "labels_train = np.array(Xy_train[\"class\"]).tolist()\n",
    "labels_val = np.array(Xy_test[\"class\"]).tolist()[:n_items_for_test//2]\n",
    "#print(labels_train)\n",
    "#print(partition_val)\n",
    "# Generators\n",
    "training_generator   = DataGenerator(partition_train, labels_train, **params)\n",
    "validation_generator = DataGenerator(partition_val, labels_val, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# partition_train.shape\n",
    "# len(training_generator)\n",
    "# np.empty((32, 2, 1))\n",
    "# partition_train\n",
    "# training_generator[1]\n",
    "# np.empty((32), dtype=int)\n",
    "X, y = training_generator[0]\n",
    "# print(y)\n",
    "#validation_generator[0]\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model on dataset\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                              validation_data=validation_generator,\n",
    "                              #steps_per_epoch=100,\n",
    "                              #use_multiprocessing=True, workers=4,\n",
    "                              epochs=epochs, shuffle=True,\n",
    "                              verbose=2,\n",
    "                              callbacks=[early_stopping, model_checkpoint])\n",
    "# ================================================================================= <-\n",
    "\n",
    "# history = model.fit(sequences_matrix_train, Xy_train[\"class\"], \n",
    "#                  validation_data=(sequences_matrix_test[:n_items_for_test//2], \n",
    "#                                   Xy_test[\"class\"][:n_items_for_test//2]), \n",
    "#                  epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "#                  callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParaEMBEDDING_DIMms[\"figure.figsize\"] = (10, 8) \n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(dgclasshistory.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(sequences_matrix_test[n_items_for_test//2:])\n",
    "pred.shape\n",
    "bin_pred, real = [], np.array(Xy_test[\"class\"]).tolist()[n_items_for_test//2:]\n",
    "for i in range(len(pred)):\n",
    "    #print(f'pred = {int(round(float(pred[i][0]), 0))},\\t real = {real[i]}')\n",
    "    bin_pred.append(int(round(float(pred[i][0]), 0))) \n",
    "#np.array(Xy_test[\"class\"]).tolist()\n",
    "bin_pred, real\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(real, bin_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(real, bin_pred), columns=['Pos', 'Neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
