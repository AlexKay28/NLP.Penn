{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from hyperas.distributions import choice, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    Train = pd.read_csv('../train_data_2.csv')[:400]\n",
    "    Validation = pd.read_csv('../validation_data_2.csv')[:100]\n",
    "    Test = pd.read_csv('../test_data_2.csv')[:100]\n",
    "\n",
    "    Train = pd.concat([Train, Validation])\n",
    "\n",
    "    X_train, y_train = Train['tweet'], to_categorical(Train['class'])\n",
    "    X_test, y_test   = Test['tweet'], to_categorical(Test['class'])\n",
    "\n",
    "    train_texts = X_train.values\n",
    "    train_texts = [s.lower() for s in train_texts]\n",
    "\n",
    "    test_texts = X_test.values\n",
    "    test_texts = [s.lower() for s in test_texts]\n",
    "\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "    char_dict = {}\n",
    "    for i, char in enumerate(alphabet):\n",
    "        char_dict[char] = i + 1\n",
    "\n",
    "    tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "    tk.fit_on_texts(train_texts)\n",
    "    tk.word_index = char_dict.copy()\n",
    "    tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "\n",
    "    train_sequences = tk.texts_to_sequences(train_texts)\n",
    "    test_sequences  = tk.texts_to_sequences(test_texts)\n",
    "\n",
    "    # Padding\n",
    "    train_data = pad_sequences(train_sequences, maxlen=200, padding='post')\n",
    "    test_data = pad_sequences(test_sequences, maxlen=200, padding='post')\n",
    "\n",
    "    # Convert to numpy array\n",
    "    x_train = np.array(train_data, dtype='float32')\n",
    "    x_test  = np.array(test_data,  dtype='float32')\n",
    "\n",
    "    char_vocab = len(tk.word_index)\n",
    "    embedding_weights = []\n",
    "    embedding_weights.append(np.zeros(char_vocab))\n",
    "\n",
    "    for char, i in tk.word_index.items():\n",
    "        onehot = np.zeros(char_vocab)\n",
    "        onehot[i-1] = 1\n",
    "        embedding_weights.append(onehot)\n",
    "    embedding_weights = np.array(embedding_weights)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test, embedding_weights):\n",
    "\n",
    "    input_size = x_train.shape[1]\n",
    "    embedding_size = embedding_weights.shape[1]\n",
    "\n",
    "    conv_layers = [[256,7, 3],\n",
    "                   [256,7, 3],\n",
    "                   [256,7, -1],\n",
    "                   [256,7, 3]] #filter_num, filter_size, pooling_size\n",
    "\n",
    "    fully_connected_layers = [200, 200]\n",
    "    num_of_classes = 2\n",
    "    dropout_p = 0.5\n",
    "\n",
    "    embedding_layer = Embedding(char_vocab+1,\n",
    "                           embedding_size,\n",
    "                           input_length=input_size,\n",
    "                           weights=[embedding_weights])\n",
    "\n",
    "    inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "    x = embedding_layer(inputs)\n",
    "    for filter_num, filter_size, pooling_size in conv_layers:\n",
    "        x = Conv1D(filter_num, filter_size)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        if pooling_size != -1:\n",
    "            x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense({{choice([256, 512, 1024])}}, activation={{choice(['relu', 'sigmoid'])}})(x)\n",
    "    x = Dropout({{uniform(0, 1)}})(x)\n",
    "    x = Dense({{choice([256, 512, 1024])}}, activation={{choice(['relu', 'sigmoid'])}})(x)\n",
    "    x = Dropout({{uniform(0, 1)}})(x)\n",
    "    predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer={{choice(['rmsprop', 'adam', 'sgd'])}},\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #model.summary()\n",
    "    result = model.fit(x_train, y_train,\n",
    "              batch_size={{choice([32, 64, 128])}},\n",
    "              epochs=5,\n",
    "              verbose=0,\n",
    "              validation_split=0.1)\n",
    "    #get the highest validation accuracy of the training epochs\n",
    "    validation_acc = np.amax(result.history['val_accuracy'])\n",
    "    print('Best validation acc of epoch: ', validation_acc)\n",
    "    return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import roc_auc_score\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.text import Tokenizer\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing.sequence import pad_sequences\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model, Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.utils import shuffle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Dropout, Activation\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adadelta, Adam, rmsprop\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'Dense': hp.choice('Dense', [256, 512, 1024]),\n",
      "        'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense_1': hp.choice('Dense_1', [256, 512, 1024]),\n",
      "        'activation_1': hp.choice('activation_1', ['relu', 'sigmoid']),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: Train = pd.read_csv('../train_data_2.csv')[:400]\n",
      "   3: Validation = pd.read_csv('../validation_data_2.csv')[:100]\n",
      "   4: Test = pd.read_csv('../test_data_2.csv')[:100]\n",
      "   5: \n",
      "   6: Train = pd.concat([Train, Validation])\n",
      "   7: \n",
      "   8: X_train, y_train = Train['tweet'], to_categorical(Train['class'])\n",
      "   9: X_test, y_test   = Test['tweet'], to_categorical(Test['class'])\n",
      "  10: \n",
      "  11: train_texts = X_train.values\n",
      "  12: train_texts = [s.lower() for s in train_texts]\n",
      "  13: \n",
      "  14: test_texts = X_test.values\n",
      "  15: test_texts = [s.lower() for s in test_texts]\n",
      "  16: \n",
      "  17: alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
      "  18: char_dict = {}\n",
      "  19: for i, char in enumerate(alphabet):\n",
      "  20:     char_dict[char] = i + 1\n",
      "  21: \n",
      "  22: tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
      "  23: tk.fit_on_texts(train_texts)\n",
      "  24: tk.word_index = char_dict.copy()\n",
      "  25: tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
      "  26: \n",
      "  27: train_sequences = tk.texts_to_sequences(train_texts)\n",
      "  28: test_sequences  = tk.texts_to_sequences(test_texts)\n",
      "  29: \n",
      "  30: # Padding\n",
      "  31: train_data = pad_sequences(train_sequences, maxlen=200, padding='post')\n",
      "  32: test_data = pad_sequences(test_sequences, maxlen=200, padding='post')\n",
      "  33: \n",
      "  34: # Convert to numpy array\n",
      "  35: x_train = np.array(train_data, dtype='float32')\n",
      "  36: x_test  = np.array(test_data,  dtype='float32')\n",
      "  37: \n",
      "  38: char_vocab = len(tk.word_index)\n",
      "  39: embedding_weights = []\n",
      "  40: embedding_weights.append(np.zeros(char_vocab))\n",
      "  41: \n",
      "  42: for char, i in tk.word_index.items():\n",
      "  43:     onehot = np.zeros(char_vocab)\n",
      "  44:     onehot[i-1] = 1\n",
      "  45:     embedding_weights.append(onehot)\n",
      "  46: embedding_weights = np.array(embedding_weights)\n",
      "  47: \n",
      "  48: \n",
      "  49: \n",
      "  50: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3: \n",
      "   4:     input_size = x_train.shape[1]\n",
      "   5:     embedding_size = embedding_weights.shape[1]\n",
      "   6: \n",
      "   7:     conv_layers = [[256,7, 3],\n",
      "   8:                    [256,7, 3],\n",
      "   9:                    [256,7, -1],\n",
      "  10:                    [256,7, 3]] #filter_num, filter_size, pooling_size\n",
      "  11: \n",
      "  12:     fully_connected_layers = [200, 200]\n",
      "  13:     num_of_classes = 2\n",
      "  14:     dropout_p = 0.5\n",
      "  15: \n",
      "  16:     embedding_layer = Embedding(char_vocab+1,\n",
      "  17:                            embedding_size,\n",
      "  18:                            input_length=input_size,\n",
      "  19:                            weights=[embedding_weights])\n",
      "  20: \n",
      "  21:     inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
      "  22:     x = embedding_layer(inputs)\n",
      "  23:     for filter_num, filter_size, pooling_size in conv_layers:\n",
      "  24:         x = Conv1D(filter_num, filter_size)(x)\n",
      "  25:         x = Activation('relu')(x)\n",
      "  26:         if pooling_size != -1:\n",
      "  27:             x = MaxPooling1D(pool_size=pooling_size)(x)\n",
      "  28:     x = Flatten()(x)\n",
      "  29:     x = Dense(space['Dense'], activation=space['activation'])(x)\n",
      "  30:     x = Dropout(space['Dropout'])(x)\n",
      "  31:     x = Dense(space['Dense_1'], activation=space['activation_1'])(x)\n",
      "  32:     x = Dropout(space['Dropout_1'])(x)\n",
      "  33:     predictions = Dense(num_of_classes, activation='softmax')(x)\n",
      "  34:     model = Model(inputs=inputs, outputs=predictions)\n",
      "  35:     model.compile(optimizer=space['optimizer'],\n",
      "  36:                   loss='binary_crossentropy',\n",
      "  37:                   metrics=['accuracy'])\n",
      "  38: \n",
      "  39:     #model.summary()\n",
      "  40:     result = model.fit(x_train, y_train,\n",
      "  41:               batch_size=space['batch_size'],\n",
      "  42:               epochs=5,\n",
      "  43:               verbose=0,\n",
      "  44:               validation_split=0.1)\n",
      "  45:     #get the highest validation accuracy of the training epochs\n",
      "  46:     validation_acc = np.amax(result.history['val_accuracy'])\n",
      "  47:     print('Best validation acc of epoch:', validation_acc)\n",
      "  48:     return {'loss': -validation_acc, 'status': STATUS_OK, 'model': model}\n",
      "  49: \n",
      "\r",
      "  0%|          | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation acc of epoch:                        \n",
      "0.8600000143051147                                   \n",
      " 50%|█████     | 1/2 [00:10<00:10, 10.94s/trial, best loss: -0.8600000143051147]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexkay/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation acc of epoch:                                                   \n",
      "0.8600000143051147                                                              \n",
      "100%|██████████| 2/2 [00:21<00:00, 10.55s/trial, best loss: -0.8600000143051147]\n",
      "Evalutation of best performing model:\n",
      "100/100 [==============================] - 0s 2ms/step\n",
      "[0.2809765034914017, 0.9599999785423279]\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 2, 'Dense_1': 2, 'Dropout': 0.6108763092812357, 'Dropout_1': 0.7371698374615214, 'activation': 0, 'activation_1': 0, 'batch_size': 1, 'optimizer': 2}\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=2,\n",
    "                                      trials=Trials(),\n",
    "                                      notebook_name='CNN_charLvl_hyperopt')\n",
    "X_train, y_train, X_test, y_test, embedding_weights = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "print(best_model.evaluate(X_test, y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
