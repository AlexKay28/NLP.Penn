{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional \n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "#from pymystem3 import Mystem\n",
    "#from stanfordcorenlp import StanfordCoreNLP\n",
    "import spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344266386467606528</td>\n",
       "      <td>809439366</td>\n",
       "      <td>0</td>\n",
       "      <td>depression hurts, cymbalta can help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>349220537903489025</td>\n",
       "      <td>323112996</td>\n",
       "      <td>0</td>\n",
       "      <td>@jessicama20045 right, but cipro can make thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>351421773079781378</td>\n",
       "      <td>713100330</td>\n",
       "      <td>0</td>\n",
       "      <td>@fibby1123 are you on paxil .. i need help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326594278472171520</td>\n",
       "      <td>543113070</td>\n",
       "      <td>0</td>\n",
       "      <td>@redicine the lamotrigine and sjs just made ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345567138376994816</td>\n",
       "      <td>138795534</td>\n",
       "      <td>0</td>\n",
       "      <td>have decided to skip my #humira shot today. my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    user_id  class  \\\n",
       "0  344266386467606528  809439366      0   \n",
       "1  349220537903489025  323112996      0   \n",
       "2  351421773079781378  713100330      0   \n",
       "3  326594278472171520  543113070      0   \n",
       "4  345567138376994816  138795534      0   \n",
       "\n",
       "                                               tweet  \n",
       "0                depression hurts, cymbalta can help  \n",
       "1  @jessicama20045 right, but cipro can make thin...  \n",
       "2         @fibby1123 are you on paxil .. i need help  \n",
       "3  @redicine the lamotrigine and sjs just made ch...  \n",
       "4  have decided to skip my #humira shot today. my...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('task2_en_training.tsv',delimiter='\\t')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'en' language\n"
     ]
    }
   ],
   "source": [
    "spacy_udpipe.download(\"en\")\n",
    "nlp = spacy_udpipe.load(\"en\")\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\"model.bin\", binary=True) \n",
    "#model.vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_postag(text = \"\"\"\"Wikipedia is a free online encyclopedia, created and \n",
    "                          edited by volunteers around the world.\"\"\",\n",
    "                          nlp=nlp):\n",
    "    doc = nlp(text)\n",
    "    tagged = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        pos = token.pos_\n",
    "        #pos = pos.split('=')[0].strip()\n",
    "        if pos not in [\"PUNCT\"]: #[\"VERB\", \"NUM\", \"ADV\", \"NOUN\", \"ADJ\", \"AUX\", \"PRON\", \"ADP\", \"DET\", \"ADV\", \"INTJ\", \"PROPN\"]:\n",
    "            tagged.append(lemma.lower() + '_' + pos)\n",
    "        #else : print(pos)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = df['tweet']\n",
    "y_train = df['class']\n",
    "\n",
    "Xy = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# paramters of the text praprations\n",
    "n_items_for_train = 50\n",
    "n_items_for_test  = 100\n",
    "max_words         = 500 \n",
    "\n",
    "# n_items_for_train твитов для тренировки с индексом класса 1\n",
    "df_1 = Xy[Xy['class'] == 1].iloc[:n_items_for_train] \n",
    "# n_items_for_train твитов для тренировки с индексом класса 0\n",
    "df_0 = Xy[Xy['class'] == 0].iloc[:n_items_for_train] \n",
    "\n",
    "Xy_ = shuffle(pd.concat([df_0, df_1]))\n",
    "Xy_train = pd.concat([Xy_[\"tweet\"].apply(lemmatize_with_postag), Xy_[\"class\"]], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# n_items_for_test твитов для теста с индексом класса 1\n",
    "df_1 = Xy[Xy['class'] == 1].iloc[n_items_for_train:n_items_for_train+n_items_for_test]\n",
    "# n_items_for_test твитов для теста с индексом класса 0\n",
    "df_0 = Xy[Xy['class'] == 0].iloc[n_items_for_train:n_items_for_train+n_items_for_test]\n",
    "\n",
    "Xy_ = shuffle(pd.concat([df_0, df_1]))\n",
    "Xy_test = pd.concat([Xy_[\"tweet\"].apply(lemmatize_with_postag), Xy_[\"class\"]], axis = 1)\n",
    "\n",
    "print(Xy_train.shape)\n",
    "print(Xy_test.shape)\n",
    "#Xy_train, Xy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>[no_DET, lamotrigine_NOUN, study_NOUN, have_AU...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>[@cotoncity_mayor_NOUN, i_PRON, get_VERB, traz...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[so_ADV, 30_NUM, -_SYM, 60_NUM, mg_NOUN, of_AD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>[i_PRON, seem_VERB, to_PART, need_VERB, to_PAR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1191</th>\n",
       "      <td>[rt_NOUN, @silkius_NOUN, @ouch_uk_NOUN, do_AUX...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>[suck_VERB, effexor_NOUN, 's_PART, dick_NOUN, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>[parent_NOUN, 452/1000_NUM, &amp;_CCONJ, amp_NOUN,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>[day_NOUN, 4_NUM, the_DET, initial_ADJ, lamotr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>[rt_NOUN, @vyvanseswag_NOUN, i_PRON, do_AUX, n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>[rt_ADJ, @jean_NOUN, _latheef_NOUN, dhen_DET, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweet  class\n",
       "74    [no_DET, lamotrigine_NOUN, study_NOUN, have_AU...      0\n",
       "106   [@cotoncity_mayor_NOUN, i_PRON, get_VERB, traz...      0\n",
       "50    [so_ADV, 30_NUM, -_SYM, 60_NUM, mg_NOUN, of_AD...      0\n",
       "1169  [i_PRON, seem_VERB, to_PART, need_VERB, to_PAR...      1\n",
       "1191  [rt_NOUN, @silkius_NOUN, @ouch_uk_NOUN, do_AUX...      1\n",
       "...                                                 ...    ...\n",
       "127   [suck_VERB, effexor_NOUN, 's_PART, dick_NOUN, ...      0\n",
       "131   [parent_NOUN, 452/1000_NUM, &_CCONJ, amp_NOUN,...      0\n",
       "1424  [day_NOUN, 4_NUM, the_DET, initial_ADJ, lamotr...      1\n",
       "1232  [rt_NOUN, @vyvanseswag_NOUN, i_PRON, do_AUX, n...      1\n",
       "59    [rt_ADJ, @jean_NOUN, _latheef_NOUN, dhen_DET, ...      0\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 33\n"
     ]
    }
   ],
   "source": [
    "# replcade as parameter in the begining\n",
    "#max_words = 2000 # ограничение вокуабуляра\n",
    "\n",
    "maxima = 0\n",
    "for ar in Xy_train['tweet'].to_list():\n",
    "    if len(ar)>maxima: maxima = len(ar)\n",
    "for ar in Xy_test['tweet']:\n",
    "    if len(ar)>maxima: maxima = len(ar)\n",
    "max_len = maxima + 1\n",
    "print(f'max_len = {max_len}')\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(Xy_train[\"tweet\"]) #fit on train\n",
    "\n",
    "# exctrax features from train\n",
    "sequences_train = tok.texts_to_sequences(Xy_train[\"tweet\"])\n",
    "sequences_matrix_train = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "\n",
    "# exctrax features from test\n",
    "sequences_test = tok.texts_to_sequences(Xy_test[\"tweet\"])\n",
    "sequences_matrix_test = sequence.pad_sequences(sequences_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 33)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is OK\n",
      "839\n"
     ]
    }
   ],
   "source": [
    "def fix_word_key(string):\n",
    "    upper_part = (re.findall('_.*',string))[0].upper()\n",
    "    result = re.sub(r'_.*', upper_part, string)\n",
    "    #print(f'was {string} -> became {result}')\n",
    "    return result\n",
    "\n",
    "word_index = tok.word_index\n",
    "#print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "wi1 = len(word_index)\n",
    "\n",
    "# fix tokenizer problem\n",
    "for key in word_index.keys():\n",
    "    fixed_key = fix_word_key(key)\n",
    "    word_index[fixed_key] = word_index.pop(key)\n",
    "    \n",
    "#print('Found %s unique tokens' % len(word_index))\n",
    "wi2 = len(word_index)\n",
    "\n",
    "if wi1 != wi2:\n",
    "    print()\n",
    "    error = 'lenght of word_index was changed!'\n",
    "    print(error.upper())\n",
    "    raise ValueError\n",
    "else:\n",
    "    print('Everything is OK')\n",
    "    print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words = 501\n",
      "(501, 300)\n",
      "take_VERB 10\n",
      "day_NOUN 23\n",
      "make_VERB 24\n",
      "rt_NOUN 25\n",
      "help_VERB 29\n",
      "still_ADV 31\n",
      "give_VERB 33\n",
      "get_VERB 34\n",
      "sleep_NOUN 35\n",
      "try_VERB 37\n",
      "weight_NOUN 38\n",
      "diary_NOUN 40\n",
      "go_VERB 41\n",
      "wake_VERB 42\n",
      "need_VERB 46\n",
      "fuck_VERB 47\n",
      "mg_NOUN 48\n",
      "side_NOUN 52\n",
      "effect_NOUN 53\n",
      "leg_NOUN 58\n",
      "feel_VERB 59\n",
      "night_NOUN 60\n",
      "know_VERB 62\n",
      "17_NUM 68\n",
      "depression_NOUN 70\n",
      "cause_VERB 73\n",
      "gain_NOUN 76\n",
      "much_ADV 77\n",
      "good_ADJ 81\n",
      "week_NOUN 82\n",
      "think_VERB 84\n",
      "dr_NOUN 88\n",
      "time_NOUN 90\n",
      "16_NUM 91\n",
      "like_VERB 92\n",
      "gain_VERB 93\n",
      "thing_NOUN 95\n",
      "hour_NOUN 97\n",
      "really_ADV 99\n",
      "start_VERB 100\n",
      "dream_NOUN 101\n",
      "hip_NOUN 102\n",
      "ach_NOUN 103\n",
      "300_NUM 104\n",
      "morning_NOUN 107\n",
      "vision_NOUN 108\n",
      "might_AUX 109\n",
      "someone_PRON 111\n",
      "lozenge_NOUN 112\n",
      "risk_NOUN 114\n",
      "infection_NOUN 115\n",
      "eat_VERB 117\n",
      "always_ADV 118\n",
      "like_ADP 119\n",
      "never_ADV 121\n",
      "pm_NOUN 123\n",
      "pain_NOUN 124\n",
      "neck_NOUN 125\n",
      "month_NOUN 127\n",
      "weird_ADJ 130\n",
      "rash_NOUN 132\n",
      "amp_NOUN 135\n",
      "chest_NOUN 136\n",
      "great_ADJ 139\n",
      "stone_NOUN 140\n",
      "trial_NOUN 144\n",
      "nightmare_NOUN 146\n",
      "nothing_PRON 148\n",
      "suit_VERB 149\n",
      "severe_ADJ 150\n",
      "switch_VERB 151\n",
      "dose_NOUN 153\n",
      "also_ADV 155\n",
      "wow_NOUN 156\n",
      "knock_VERB 157\n",
      "wait_VERB 158\n",
      "new_ADJ 160\n",
      "right_ADV 161\n",
      "like_SCONJ 165\n",
      "hate_VERB 167\n",
      "1945_NUM 172\n",
      "doctor_NOUN 173\n",
      "horrid_ADJ 177\n",
      "today_NOUN 178\n",
      "treatment_NOUN 181\n",
      "pill_NOUN 184\n",
      "continue_VERB 185\n",
      "body_NOUN 186\n",
      "call_VERB 187\n",
      "least_ADV 188\n",
      "caffeine_NOUN 189\n",
      "addiction_NOUN 190\n",
      "already_ADV 191\n",
      "50_NUM 192\n",
      "see_VERB 193\n",
      "generic_NOUN 194\n",
      "23_NUM 196\n",
      "yesterday_NOUN 197\n",
      "intense_ADJ 199\n",
      "150_NUM 200\n",
      "speak_VERB 203\n",
      "psychiatrist_NOUN 204\n",
      "stop_VERB 206\n",
      "interfere_VERB 208\n",
      "memory_NOUN 209\n",
      "loss_NOUN 210\n",
      "thank_VERB 211\n",
      "god_NOUN 212\n",
      "tea_NOUN 236\n",
      "sierra_NOUN 238\n",
      "mist_NOUN 239\n",
      "wish_VERB 240\n",
      "two_NUM 243\n",
      "antidepressant_NOUN 244\n",
      "link_VERB 246\n",
      "significantly_ADV 247\n",
      "increase_VERB 248\n",
      "friend_NOUN 252\n",
      "anybody_PRON 253\n",
      "complication_NOUN 255\n",
      "big_ADJ 256\n",
      "rupture_NOUN 258\n",
      "figure_VERB 259\n",
      "bed_NOUN 261\n",
      "hole_NOUN 262\n",
      "head_NOUN 263\n",
      "awful_ADJ 265\n",
      "spray_VERB 266\n",
      "tongue_NOUN 268\n",
      "strong_ADJ 270\n",
      "word_NOUN 271\n",
      "permanent_ADJ 274\n",
      "dry_NOUN 275\n",
      "mouth_NOUN 276\n",
      "third_ADJ 281\n",
      "visit_NOUN 282\n",
      "badly_ADV 287\n",
      "05.00_NUM 288\n",
      "almost_ADV 290\n",
      "daily_ADJ 291\n",
      "past_ADJ 293\n",
      "though_SCONJ 296\n",
      "rheum_NOUN 297\n",
      "real_ADJ 298\n",
      "light_NOUN 299\n",
      "bastard_NOUN 300\n",
      "bring_VERB 302\n",
      "world_NOUN 306\n",
      "yup_INTJ 317\n",
      "41_NUM 319\n",
      "order_VERB 321\n",
      "culture_NOUN 323\n",
      "specific_ADJ 324\n",
      "result_NOUN 326\n",
      "determined_ADJ 327\n",
      "cure_VERB 328\n",
      "weather_NOUN 331\n",
      "put_VERB 332\n",
      "trash_NOUN 333\n",
      "even_ADV 334\n",
      "abdomen_NOUN 343\n",
      "benefit_NOUN 346\n",
      "psoriasis_NOUN 347\n",
      "pic_NOUN 348\n",
      "question_NOUN 349\n",
      "recently_ADV 350\n",
      "hyper_ADJ 351\n",
      "sensitive_ADJ 352\n",
      "different_ADJ 353\n",
      "anyone_PRON 355\n",
      "relate_VERB 358\n",
      "lb_NOUN 359\n",
      "clock_VERB 361\n",
      "add_VERB 362\n",
      "since_ADP 363\n",
      "accord_VERB 365\n",
      "bmi_NOUN 366\n",
      "obese_ADJ 367\n",
      "clearly_ADV 368\n",
      "drug_NOUN 370\n",
      "inhibitor_NOUN 373\n",
      "show_VERB 374\n",
      "reduce_VERB 376\n",
      "cardiovascular_ADJ 377\n",
      "large_ADJ 378\n",
      "clinical_ADJ 379\n",
      "partly_ADV 382\n",
      "true_ADJ 383\n",
      "ever_ADV 385\n",
      "carb_NOUN 387\n",
      "craving_NOUN 388\n",
      "unfortunate_ADJ 389\n",
      "swear_VERB 394\n",
      "horrible_ADJ 396\n",
      "yeah_INTJ 398\n",
      "work_VERB 399\n",
      "anything_PRON 401\n",
      "cope_VERB 402\n",
      "miss_NOUN 404\n",
      "carbamazepine_NOUN 407\n",
      "surgery_NOUN 413\n",
      "programme_VERB 414\n",
      "tremor_NOUN 417\n",
      "well_ADV 420\n",
      "hit_VERB 422\n",
      "wall_NOUN 423\n",
      "one_NUM 424\n",
      "cpa_NOUN 427\n",
      "uh_INTJ 431\n",
      "oh_INTJ 432\n",
      "low_ADJ 437\n",
      "anorexia_NOUN 441\n",
      "stimulate_VERB 442\n",
      "nausea_NOUN 444\n",
      "constipation_NOUN 445\n",
      "sedation_NOUN 446\n",
      "sex_NOUN 449\n",
      "say_VERB 451\n",
      "tweet_VERB 455\n",
      "send_VERB 456\n",
      "affordable_ADJ 458\n",
      "sleep_VERB 464\n",
      "must_AUX 468\n",
      "sedative_ADJ 469\n",
      "separately_ADV 471\n",
      "news_NOUN 475\n",
      "one_NOUN 477\n",
      "particular_ADJ 478\n",
      "hinde_NOUN 481\n",
      "relieve_VERB 483\n",
      "symptom_NOUN 484\n",
      "best_ADJ 486\n",
      "number_NOUN 488\n",
      "watch_VERB 489\n",
      "withdrawal_NOUN 491\n",
      "feed_VERB 499\n",
      "ciprofloxacin_NOUN 216\n",
      "speech_NOUN 217\n",
      "circuit_NOUN 218\n",
      "harmony_NOUN 219\n",
      "mere_ADJ 220\n",
      "chance_NOUN 221\n",
      "instance_NOUN 224\n",
      "herbaceous_ADJ 225\n",
      "graceful_ADJ 226\n",
      "usual_ADJ 230\n",
      "come_VERB 232\n",
      "throat_NOUN 235\n",
      "Null word embeddings: 253\n"
     ]
    }
   ],
   "source": [
    "# делаем Embedding на основе w2v модели\n",
    "\n",
    "nb_words = min(max_words, len(word_index))+1 # проверяем где меньше, в нашем датасете или в токенайзере.\n",
    "EMBEDDING_DIM = 300 # размерность векторов в нашей модели w2v\n",
    "\n",
    "print(f'number of words = {nb_words}')\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "counter = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words: continue\n",
    "    if word in w2v_model.vocab:\n",
    "        print(word, i)\n",
    "        embedding_matrix[i] = w2v_model[word]\n",
    "        #break\n",
    "#     if counter > nb_words: break\n",
    "#     counter += 1\n",
    "\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "#pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BI-LSTM (3.0)\n",
    "\n",
    "https://stackoverflow.com/questions/44924690/keras-the-difference-between-lstm-dropout-and-lstm-recurrent-dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_drop_lstm  = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "act = 'relu'\n",
    "\n",
    "\n",
    "embedding_layer      = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=False) \n",
    "lstm_layer           = Bidirectional(LSTM(64, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm))\n",
    "\n",
    "# CONFIGURATION OF BI-LSTM\n",
    "sequence_1_input     = Input(shape=(max_len,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1                   = lstm_layer(embedded_sequences_1)\n",
    "merged               = Dropout(rate_drop_dense)(x1)\n",
    "merged               = BatchNormalization()(merged)\n",
    "merged               = Dense(32, activation=act)(merged)\n",
    "merged               = Dropout(rate_drop_dense)(merged)\n",
    "merged               = BatchNormalization()(merged)\n",
    "preds                = Dense(1, activation='sigmoid')(merged)\n",
    "#preds                = Dense(2, activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 33)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 33, 300)           150300    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               186880    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 341,981\n",
      "Trainable params: 191,361\n",
      "Non-trainable params: 150,620\n",
      "_________________________________________________________________\n",
      "lstm_10_10_0.23_0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(10, 10, rate_drop_lstm, \\\n",
    "        rate_drop_dense)\n",
    "\n",
    "model = Model(inputs=[sequence_1_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "print(STAMP, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from myclass import DataGenerator\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "patience = 20\n",
    "\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=patience)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "# ================================================================================= ->\n",
    "# Parameters\n",
    "params = {'batch_size': batch_size, #(n_items_for_train, max_len, EMBEDDING_DIM), # (number of channels, number of classes, batch size)\n",
    "          'dim': max_len,\n",
    "          'n_classes': 1,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "\n",
    "np.array([[index, seq] for index, seq in enumerate(sequences_matrix_train.tolist())])\n",
    "np.array([[index, seq] for index, seq in enumerate(sequences_matrix_test.tolist())])\n",
    "\n",
    "# IDs    {'train': ['id-1', 'id-2', 'id-3'], 'validation': ['id-4']}\n",
    "partition_train = sequences_matrix_train#.tolist()\n",
    "partition_val   = sequences_matrix_test[:n_items_for_test//2] #.tolist()[:n_items_for_test//2]\n",
    "partition_test  = sequences_matrix_test[n_items_for_test//2:] #.tolist()[n_items_for_test//2:]\n",
    "\n",
    "# Labels {'id-1': 0, 'id-2': 1, 'id-3': 2, 'id-4': 1}\n",
    "labels_train = np.array(Xy_train[\"class\"]).tolist()\n",
    "labels_val = np.array(Xy_test[\"class\"]).tolist()[:n_items_for_test//2]\n",
    "#print(labels_train)\n",
    "#print(partition_val)\n",
    "# Generators\n",
    "training_generator   = DataGenerator(partition_train, labels_train, **params)\n",
    "validation_generator = DataGenerator(partition_val, labels_val, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# partition_train.shape\n",
    "# len(training_generator)\n",
    "# np.empty((32, 2, 1))\n",
    "# partition_train\n",
    "# training_generator[1]\n",
    "# np.empty((32), dtype=int)\n",
    "X, y = training_generator[0]\n",
    "# print(y)\n",
    "#validation_generator[0]\n",
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 2s - loss: 0.9810 - acc: 0.4792 - val_loss: 0.6943 - val_acc: 0.4375\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.7543 - acc: 0.6250 - val_loss: 0.6893 - val_acc: 0.5312\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.6610 - acc: 0.6667 - val_loss: 0.6986 - val_acc: 0.3750\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.5439 - acc: 0.7500 - val_loss: 0.6959 - val_acc: 0.4062\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.4460 - acc: 0.7917 - val_loss: 0.6913 - val_acc: 0.5000\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.5603 - acc: 0.6979 - val_loss: 0.6910 - val_acc: 0.5938\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.4941 - acc: 0.7292 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.4012 - acc: 0.8229 - val_loss: 0.6912 - val_acc: 0.5312\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.4627 - acc: 0.7500 - val_loss: 0.6914 - val_acc: 0.5000\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.4317 - acc: 0.7917 - val_loss: 0.6924 - val_acc: 0.5625\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.3889 - acc: 0.8542 - val_loss: 0.6876 - val_acc: 0.5938\n",
      "Epoch 12/200\n",
      " - 0s - loss: 0.3880 - acc: 0.7917 - val_loss: 0.6939 - val_acc: 0.5625\n",
      "Epoch 13/200\n",
      " - 0s - loss: 0.3031 - acc: 0.8646 - val_loss: 0.6830 - val_acc: 0.7188\n",
      "Epoch 14/200\n",
      " - 0s - loss: 0.2999 - acc: 0.8958 - val_loss: 0.6912 - val_acc: 0.5938\n",
      "Epoch 15/200\n",
      " - 0s - loss: 0.3163 - acc: 0.9062 - val_loss: 0.6936 - val_acc: 0.5625\n",
      "Epoch 16/200\n",
      " - 0s - loss: 0.3061 - acc: 0.8542 - val_loss: 0.6849 - val_acc: 0.5625\n",
      "Epoch 17/200\n",
      " - 0s - loss: 0.2563 - acc: 0.9062 - val_loss: 0.6885 - val_acc: 0.5625\n",
      "Epoch 18/200\n",
      " - 0s - loss: 0.2296 - acc: 0.8958 - val_loss: 0.6858 - val_acc: 0.5625\n",
      "Epoch 19/200\n",
      " - 0s - loss: 0.2232 - acc: 0.8958 - val_loss: 0.6879 - val_acc: 0.5625\n",
      "Epoch 20/200\n",
      " - 0s - loss: 0.3011 - acc: 0.9062 - val_loss: 0.6868 - val_acc: 0.5312\n",
      "Epoch 21/200\n",
      " - 0s - loss: 0.3023 - acc: 0.8854 - val_loss: 0.6802 - val_acc: 0.6875\n",
      "Epoch 22/200\n",
      " - 0s - loss: 0.2512 - acc: 0.8854 - val_loss: 0.6903 - val_acc: 0.6250\n",
      "Epoch 23/200\n",
      " - 0s - loss: 0.1895 - acc: 0.9479 - val_loss: 0.6851 - val_acc: 0.5625\n",
      "Epoch 24/200\n",
      " - 0s - loss: 0.2112 - acc: 0.9167 - val_loss: 0.6782 - val_acc: 0.5938\n",
      "Epoch 25/200\n",
      " - 0s - loss: 0.2016 - acc: 0.9375 - val_loss: 0.6626 - val_acc: 0.6875\n",
      "Epoch 26/200\n",
      " - 0s - loss: 0.1587 - acc: 0.9479 - val_loss: 0.6791 - val_acc: 0.5938\n",
      "Epoch 27/200\n",
      " - 0s - loss: 0.1592 - acc: 0.9583 - val_loss: 0.6806 - val_acc: 0.6250\n",
      "Epoch 28/200\n",
      " - 0s - loss: 0.1658 - acc: 0.9583 - val_loss: 0.6928 - val_acc: 0.5625\n",
      "Epoch 29/200\n",
      " - 0s - loss: 0.2177 - acc: 0.9167 - val_loss: 0.6798 - val_acc: 0.5625\n",
      "Epoch 30/200\n",
      " - 0s - loss: 0.1325 - acc: 0.9479 - val_loss: 0.6518 - val_acc: 0.6875\n",
      "Epoch 31/200\n",
      " - 0s - loss: 0.1060 - acc: 0.9792 - val_loss: 0.7107 - val_acc: 0.5000\n",
      "Epoch 32/200\n",
      " - 0s - loss: 0.1463 - acc: 0.9479 - val_loss: 0.6839 - val_acc: 0.5625\n",
      "Epoch 33/200\n",
      " - 0s - loss: 0.1605 - acc: 0.9375 - val_loss: 0.6744 - val_acc: 0.5938\n",
      "Epoch 34/200\n",
      " - 0s - loss: 0.1618 - acc: 0.9271 - val_loss: 0.6979 - val_acc: 0.5312\n",
      "Epoch 35/200\n",
      " - 0s - loss: 0.1096 - acc: 0.9688 - val_loss: 0.6887 - val_acc: 0.5312\n",
      "Epoch 36/200\n",
      " - 0s - loss: 0.1396 - acc: 0.9479 - val_loss: 0.6932 - val_acc: 0.5625\n",
      "Epoch 37/200\n",
      " - 0s - loss: 0.1919 - acc: 0.9375 - val_loss: 0.7079 - val_acc: 0.5000\n",
      "Epoch 38/200\n",
      " - 0s - loss: 0.1054 - acc: 0.9688 - val_loss: 0.6692 - val_acc: 0.6875\n",
      "Epoch 39/200\n",
      " - 0s - loss: 0.1305 - acc: 0.9479 - val_loss: 0.6640 - val_acc: 0.5938\n",
      "Epoch 40/200\n",
      " - 0s - loss: 0.1585 - acc: 0.9167 - val_loss: 0.6751 - val_acc: 0.5938\n",
      "Epoch 41/200\n",
      " - 0s - loss: 0.1288 - acc: 0.9583 - val_loss: 0.7015 - val_acc: 0.5625\n",
      "Epoch 42/200\n",
      " - 0s - loss: 0.1159 - acc: 0.9583 - val_loss: 0.7158 - val_acc: 0.5000\n",
      "Epoch 43/200\n",
      " - 0s - loss: 0.0970 - acc: 0.9583 - val_loss: 0.6782 - val_acc: 0.5625\n",
      "Epoch 44/200\n",
      " - 0s - loss: 0.0873 - acc: 0.9896 - val_loss: 0.6701 - val_acc: 0.6562\n",
      "Epoch 45/200\n",
      " - 0s - loss: 0.1247 - acc: 0.9479 - val_loss: 0.6669 - val_acc: 0.6250\n",
      "Epoch 46/200\n",
      " - 0s - loss: 0.1113 - acc: 0.9583 - val_loss: 0.6897 - val_acc: 0.5938\n",
      "Epoch 47/200\n",
      " - 0s - loss: 0.0749 - acc: 0.9896 - val_loss: 0.6450 - val_acc: 0.6875\n",
      "Epoch 48/200\n",
      " - 0s - loss: 0.1488 - acc: 0.9583 - val_loss: 0.6669 - val_acc: 0.6250\n",
      "Epoch 49/200\n",
      " - 0s - loss: 0.1254 - acc: 0.9688 - val_loss: 0.6971 - val_acc: 0.5938\n",
      "Epoch 50/200\n",
      " - 0s - loss: 0.0941 - acc: 0.9583 - val_loss: 0.6872 - val_acc: 0.5938\n",
      "Epoch 51/200\n",
      " - 0s - loss: 0.0969 - acc: 0.9583 - val_loss: 0.6978 - val_acc: 0.5938\n",
      "Epoch 52/200\n",
      " - 0s - loss: 0.0979 - acc: 0.9583 - val_loss: 0.6679 - val_acc: 0.6562\n",
      "Epoch 53/200\n",
      " - 0s - loss: 0.1702 - acc: 0.9271 - val_loss: 0.6681 - val_acc: 0.6562\n",
      "Epoch 54/200\n",
      " - 0s - loss: 0.0467 - acc: 0.9896 - val_loss: 0.6693 - val_acc: 0.6562\n",
      "Epoch 55/200\n",
      " - 0s - loss: 0.0995 - acc: 0.9688 - val_loss: 0.6637 - val_acc: 0.6250\n",
      "Epoch 56/200\n",
      " - 0s - loss: 0.0711 - acc: 0.9896 - val_loss: 0.6523 - val_acc: 0.6875\n",
      "Epoch 57/200\n",
      " - 0s - loss: 0.1055 - acc: 0.9688 - val_loss: 0.7941 - val_acc: 0.5312\n",
      "Epoch 58/200\n",
      " - 0s - loss: 0.0564 - acc: 0.9688 - val_loss: 0.7489 - val_acc: 0.5312\n",
      "Epoch 59/200\n",
      " - 0s - loss: 0.0769 - acc: 0.9792 - val_loss: 0.7071 - val_acc: 0.6250\n",
      "Epoch 60/200\n",
      " - 0s - loss: 0.0687 - acc: 0.9688 - val_loss: 0.7042 - val_acc: 0.5625\n",
      "Epoch 61/200\n",
      " - 0s - loss: 0.1354 - acc: 0.9583 - val_loss: 0.6814 - val_acc: 0.5938\n",
      "Epoch 62/200\n",
      " - 0s - loss: 0.1654 - acc: 0.9271 - val_loss: 0.6543 - val_acc: 0.6562\n",
      "Epoch 63/200\n",
      " - 0s - loss: 0.1345 - acc: 0.9583 - val_loss: 0.6810 - val_acc: 0.5938\n",
      "Epoch 64/200\n",
      " - 0s - loss: 0.0622 - acc: 0.9896 - val_loss: 0.7367 - val_acc: 0.5625\n",
      "Epoch 65/200\n",
      " - 0s - loss: 0.1405 - acc: 0.9479 - val_loss: 0.5851 - val_acc: 0.7188\n",
      "Epoch 66/200\n",
      " - 0s - loss: 0.1024 - acc: 0.9688 - val_loss: 0.6063 - val_acc: 0.7188\n",
      "Epoch 67/200\n",
      " - 0s - loss: 0.0738 - acc: 0.9792 - val_loss: 0.7411 - val_acc: 0.4688\n",
      "Epoch 68/200\n",
      " - 0s - loss: 0.0871 - acc: 0.9583 - val_loss: 0.6939 - val_acc: 0.6250\n",
      "Epoch 69/200\n",
      " - 0s - loss: 0.0864 - acc: 0.9792 - val_loss: 0.7167 - val_acc: 0.5625\n",
      "Epoch 70/200\n",
      " - 0s - loss: 0.0578 - acc: 0.9792 - val_loss: 0.7710 - val_acc: 0.4688\n",
      "Epoch 71/200\n",
      " - 0s - loss: 0.0437 - acc: 1.0000 - val_loss: 0.7912 - val_acc: 0.4688\n",
      "Epoch 72/200\n",
      " - 0s - loss: 0.0880 - acc: 0.9688 - val_loss: 0.7135 - val_acc: 0.5000\n",
      "Epoch 73/200\n",
      " - 0s - loss: 0.0988 - acc: 0.9792 - val_loss: 0.6292 - val_acc: 0.6562\n",
      "Epoch 74/200\n",
      " - 0s - loss: 0.0934 - acc: 0.9792 - val_loss: 0.6616 - val_acc: 0.6250\n",
      "Epoch 75/200\n",
      " - 0s - loss: 0.0957 - acc: 0.9583 - val_loss: 0.6941 - val_acc: 0.5938\n",
      "Epoch 76/200\n",
      " - 0s - loss: 0.0786 - acc: 0.9792 - val_loss: 0.6578 - val_acc: 0.6562\n",
      "Epoch 77/200\n",
      " - 0s - loss: 0.1244 - acc: 0.9479 - val_loss: 0.7156 - val_acc: 0.5938\n",
      "Epoch 78/200\n",
      " - 0s - loss: 0.0760 - acc: 0.9688 - val_loss: 0.7824 - val_acc: 0.5938\n",
      "Epoch 79/200\n",
      " - 0s - loss: 0.1370 - acc: 0.9583 - val_loss: 0.7865 - val_acc: 0.5625\n",
      "Epoch 80/200\n",
      " - 0s - loss: 0.0712 - acc: 0.9792 - val_loss: 0.7486 - val_acc: 0.5625\n",
      "Epoch 81/200\n",
      " - 0s - loss: 0.0906 - acc: 0.9688 - val_loss: 0.6489 - val_acc: 0.6875\n",
      "Epoch 82/200\n",
      " - 0s - loss: 0.1202 - acc: 0.9479 - val_loss: 0.7216 - val_acc: 0.5625\n",
      "Epoch 83/200\n",
      " - 0s - loss: 0.0817 - acc: 0.9479 - val_loss: 0.7370 - val_acc: 0.5312\n",
      "Epoch 84/200\n",
      " - 0s - loss: 0.1401 - acc: 0.9479 - val_loss: 0.6374 - val_acc: 0.6875\n",
      "Epoch 85/200\n",
      " - 0s - loss: 0.0819 - acc: 0.9688 - val_loss: 0.6574 - val_acc: 0.6562\n"
     ]
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "history = model.fit_generator(generator=training_generator,\n",
    "                              validation_data=validation_generator,\n",
    "                              #steps_per_epoch=100,\n",
    "                              #use_multiprocessing=True, workers=4,\n",
    "                              epochs=epochs, shuffle=True,\n",
    "                              verbose=2,\n",
    "                              callbacks=[early_stopping, model_checkpoint])\n",
    "# ================================================================================= <-\n",
    "\n",
    "# history = model.fit(sequences_matrix_train, Xy_train[\"class\"], \n",
    "#                  validation_data=(sequences_matrix_test[:n_items_for_test//2], \n",
    "#                                   Xy_test[\"class\"][:n_items_for_test//2]), \n",
    "#                  epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "#                  callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'rcParaEMBEDDING_DIMms'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-19e8c1a6bd96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParaEMBEDDING_DIMms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"figure.figsize\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# summarize history for accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'rcParaEMBEDDING_DIMms'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParaEMBEDDING_DIMms[\"figure.figsize\"] = (10, 8) \n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(dgclasshistory.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(sequences_matrix_test[n_items_for_test//2:])\n",
    "pred.shape\n",
    "bin_pred, real = [], np.array(Xy_test[\"class\"]).tolist()[n_items_for_test//2:]\n",
    "for i in range(len(pred)):\n",
    "    #print(f'pred = {int(round(float(pred[i][0]), 0))},\\t real = {real[i]}')\n",
    "    bin_pred.append(int(round(float(pred[i][0]), 0))) \n",
    "#np.array(Xy_test[\"class\"]).tolist()\n",
    "bin_pred, real\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(real, bin_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix(real, bin_pred), columns=['Pos', 'Neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basicnlp",
   "language": "python",
   "name": "basicnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
