{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Activation, Dense, Dropout, Input, Embedding, Bidirectional \n",
    "from keras.layers import Flatten, Conv1D, MaxPooling1D\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "#from pymystem3 import Mystem\n",
    "#from stanfordcorenlp import StanfordCoreNLP\n",
    "import spacy_udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>344266386467606528</td>\n",
       "      <td>809439366</td>\n",
       "      <td>0</td>\n",
       "      <td>depression hurts, cymbalta can help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>349220537903489025</td>\n",
       "      <td>323112996</td>\n",
       "      <td>0</td>\n",
       "      <td>@jessicama20045 right, but cipro can make thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>351421773079781378</td>\n",
       "      <td>713100330</td>\n",
       "      <td>0</td>\n",
       "      <td>@fibby1123 are you on paxil .. i need help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>326594278472171520</td>\n",
       "      <td>543113070</td>\n",
       "      <td>0</td>\n",
       "      <td>@redicine the lamotrigine and sjs just made ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345567138376994816</td>\n",
       "      <td>138795534</td>\n",
       "      <td>0</td>\n",
       "      <td>have decided to skip my #humira shot today. my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    user_id  class  \\\n",
       "0  344266386467606528  809439366      0   \n",
       "1  349220537903489025  323112996      0   \n",
       "2  351421773079781378  713100330      0   \n",
       "3  326594278472171520  543113070      0   \n",
       "4  345567138376994816  138795534      0   \n",
       "\n",
       "                                               tweet  \n",
       "0                depression hurts, cymbalta can help  \n",
       "1  @jessicama20045 right, but cipro can make thin...  \n",
       "2         @fibby1123 are you on paxil .. i need help  \n",
       "3  @redicine the lamotrigine and sjs just made ch...  \n",
       "4  have decided to skip my #humira shot today. my...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../task2_data/task2_en_training.tsv',delimiter='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'en' language\n"
     ]
    }
   ],
   "source": [
    "spacy_udpipe.download(\"en\")\n",
    "nlp = spacy_udpipe.load(\"en\")\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"../../Embeddings/model.bin\", binary=True) \n",
    "#model.vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_postag(text = \"\"\"\"Wikipedia is a free online encyclopedia, created and \n",
    "                          edited by volunteers around the world.\"\"\",\n",
    "                          nlp=nlp):\n",
    "    doc = nlp(text)\n",
    "    tagged = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        if lemma[0] in '@?':\n",
    "            continue        \n",
    "        pos = token.pos_\n",
    "        #pos = pos.split('=')[0].strip()\n",
    "        if pos not in [\"PUNCT\"]: #[\"VERB\", \"NUM\", \"ADV\", \"NOUN\", \"ADJ\", \"AUX\", \"PRON\", \"ADP\", \"DET\", \"ADV\", \"INTJ\", \"PROPN\"]:\n",
    "            tagged.append(lemma.lower() + '_' + pos)\n",
    "        #else : print(pos)\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train = df['tweet']\n",
    "y_train = df['class']\n",
    "\n",
    "Xy = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# paramters of the text praprations\n",
    "n_items_for_train = 500\n",
    "n_items_for_test  = 200\n",
    "max_words         = 4000 \n",
    "\n",
    "# n_items_for_train твитов для тренировки с индексом класса 1\n",
    "df_1 = Xy[Xy['class'] == 1].iloc[:n_items_for_train] \n",
    "# n_items_for_train твитов для тренировки с индексом класса 0\n",
    "df_0 = Xy[Xy['class'] == 0].iloc[:n_items_for_train] \n",
    "\n",
    "Xy_ = shuffle(pd.concat([df_0, df_1]))\n",
    "Xy_train = pd.concat([Xy_[\"tweet\"].apply(lemmatize_with_postag), Xy_[\"class\"]], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# n_items_for_test твитов для теста с индексом класса 1\n",
    "df_1 = Xy[Xy['class'] == 1].iloc[n_items_for_train:n_items_for_train+n_items_for_test]\n",
    "# n_items_for_test твитов для теста с индексом класса 0\n",
    "df_0 = Xy[Xy['class'] == 0].iloc[n_items_for_train:n_items_for_train+n_items_for_test]\n",
    "\n",
    "Xy_ = shuffle(pd.concat([df_0, df_1]))\n",
    "Xy_test = pd.concat([Xy_[\"tweet\"].apply(lemmatize_with_postag), Xy_[\"class\"]], axis = 1)\n",
    "\n",
    "print(Xy_train.shape)\n",
    "print(Xy_test.shape)\n",
    "#Xy_train, Xy_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4300, 2)\n",
      "(1303, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train = df['tweet']\n",
    "y_train = df['class']\n",
    "\n",
    "Xy = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# paramters of the text praprations\n",
    "n_items_for_train = 500\n",
    "n_items_for_test  = 200\n",
    "max_words         = 8000 \n",
    "\n",
    "# n_items_for_train твитов для тренировки с индексом класса 1\n",
    "df_1 = Xy[Xy['class'] == 1].iloc[:1500] \n",
    "# n_items_for_train твитов для тренировки с индексом класса 0\n",
    "df_0 = Xy[Xy['class'] == 0].iloc[:2800] \n",
    "\n",
    "Xy_ = shuffle(pd.concat([df_0, df_1]))\n",
    "Xy_train = pd.concat([Xy_[\"tweet\"].apply(lemmatize_with_postag), Xy_[\"class\"]], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# n_items_for_test твитов для теста с индексом класса 1\n",
    "df_1 = Xy[Xy['class'] == 1].iloc[1500:]\n",
    "# n_items_for_test твитов для теста с индексом класса 0\n",
    "df_0 = Xy[Xy['class'] == 0].iloc[2000:2900]\n",
    "\n",
    "Xy_ = shuffle(pd.concat([df_0, df_1]))\n",
    "Xy_test = pd.concat([Xy_[\"tweet\"].apply(lemmatize_with_postag), Xy_[\"class\"]], axis = 1)\n",
    "\n",
    "print(Xy_train.shape)\n",
    "print(Xy_test.shape)\n",
    "#Xy_train, Xy_test\n",
    "\n",
    "n_items_for_train = len(Xy_train)\n",
    "n_items_for_test = len(Xy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "print(sns.countplot(Xy_train['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sns.countplot(Xy_test['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len = 36\n"
     ]
    }
   ],
   "source": [
    "# replcade as parameter in the begining\n",
    "#max_words = 2000 # ограничение вокуабуляра\n",
    "\n",
    "maxima = 0\n",
    "for ar in Xy_train['tweet'].to_list():\n",
    "    if len(ar)>maxima: maxima = len(ar)\n",
    "for ar in Xy_test['tweet']:\n",
    "    if len(ar)>maxima: maxima = len(ar)\n",
    "max_len = maxima + 1\n",
    "print(f'max_len = {max_len}')\n",
    "\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(Xy_train[\"tweet\"]) #fit on train\n",
    "\n",
    "# exctrax features from train\n",
    "sequences_train = tok.texts_to_sequences(Xy_train[\"tweet\"])\n",
    "sequences_matrix_train = sequence.pad_sequences(sequences_train, maxlen=max_len)\n",
    "\n",
    "# exctrax features from test\n",
    "sequences_test = tok.texts_to_sequences(Xy_test[\"tweet\"])\n",
    "sequences_matrix_test = sequence.pad_sequences(sequences_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is OK\n",
      "9838\n"
     ]
    }
   ],
   "source": [
    "def fix_word_key(string):\n",
    "    upper_part = (re.findall('_.*',string))[0].upper()\n",
    "    result = re.sub(r'_.*', upper_part, string)\n",
    "    #print(f'was {string} -> became {result}')\n",
    "    return result\n",
    "\n",
    "word_index = tok.word_index\n",
    "#print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "wi1 = len(word_index)\n",
    "\n",
    "# fix tokenizer problem\n",
    "for key in word_index.keys():\n",
    "    fixed_key = fix_word_key(key)\n",
    "    word_index[fixed_key] = word_index.pop(key)\n",
    "    \n",
    "#print('Found %s unique tokens' % len(word_index))\n",
    "wi2 = len(word_index)\n",
    "\n",
    "if wi1 != wi2:\n",
    "    print()\n",
    "    error = 'lenght of word_index was changed!'\n",
    "    print(error.upper())\n",
    "    raise ValueError\n",
    "else:\n",
    "    print('Everything is OK')\n",
    "    print(len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words = 8001\n",
      "(8001, 300)\n",
      "Null word embeddings: 4904\n"
     ]
    }
   ],
   "source": [
    "# делаем Embedding на основе w2v модели\n",
    "\n",
    "nb_words = min(max_words, len(word_index))+1 # проверяем где меньше, в нашем датасете или в токенайзере.\n",
    "EMBEDDING_DIM = 300 # размерность векторов в нашей модели w2v\n",
    "\n",
    "print(f'number of words = {nb_words}')\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "counter = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words: continue\n",
    "    if word in model.vocab:\n",
    "        #print(model[word])\n",
    "        embedding_matrix[i] = model[word]\n",
    "#     if counter > nb_words: break\n",
    "#     counter += 1\n",
    "\n",
    "\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "#pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences_matrix_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "act = 'relu'\n",
    "\n",
    "embedding_layer      = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_len,\n",
    "                                trainable=False) \n",
    "\n",
    "# CONFIGURATION OF CNN\n",
    "sequence_1_input     = Input(shape=(max_len,), dtype='int32')\n",
    "x                    = embedding_layer(sequence_1_input)\n",
    "\n",
    "x                    = Conv1D(filters=300, kernel_size=3, activation='relu', input_shape=(x.shape))(x)\n",
    "x                    = Dropout(0.5)(x)\n",
    "x                    = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x                    = Conv1D(filters=300, kernel_size=5, activation='relu', input_shape=(x.shape))(x)\n",
    "x                    = Dropout(0.5)(x)\n",
    "x                    = MaxPooling1D(pool_size=1)(x)\n",
    "\n",
    "# for filter_num, filter_size, pooling_size in conv_layers:\n",
    "#     x = Conv1D(filter_num, filter_size)(x)\n",
    "#     x = Activation('relu')(x)\n",
    "#     if pooling_size != -1:\n",
    "#         x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "        \n",
    "x                    = Flatten()(x)\n",
    "\n",
    "# for dense_size in fully_connected_layers:\n",
    "#     x = Dense(dense_size, activation='relu')(x)\n",
    "#     x = Dropout(dropout_p)(x)\n",
    "\n",
    "merged               = Dropout(rate_drop_dense)(x)\n",
    "merged               = BatchNormalization()(merged)\n",
    "merged               = Dense(32, activation=act)(merged)\n",
    "merged               = Dropout(rate_drop_dense)(merged)\n",
    "merged               = BatchNormalization()(merged)\n",
    "preds                = Dense(2, activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 36, 300)           2400300   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 34, 300)           270300    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 34, 300)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 13, 300)           450300    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 300)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 13, 300)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3900)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3900)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3900)              15600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                124832    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 3,261,526\n",
      "Trainable params: 853,362\n",
      "Non-trainable params: 2,408,164\n",
      "_________________________________________________________________\n",
      "CNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STAMP = 'CNN'\n",
    "\n",
    "model = Model(inputs=[sequence_1_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['acc'])\n",
    "model.summary()\n",
    "print(STAMP, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Xy_train = to_categorical(Xy_train['class'], num_classes=2)\n",
    "Xy_test = to_categorical(Xy_test['class'], num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4300 samples, validate on 651 samples\n",
      "Epoch 1/10\n",
      "4300/4300 [==============================] - 9s 2ms/step - loss: 0.7547 - acc: 0.6253 - val_loss: 0.5945 - val_acc: 0.7127\n",
      "Epoch 2/10\n",
      "4300/4300 [==============================] - 7s 2ms/step - loss: 0.5972 - acc: 0.7098 - val_loss: 0.5672 - val_acc: 0.7127\n",
      "Epoch 3/10\n",
      "4300/4300 [==============================] - 6s 1ms/step - loss: 0.5176 - acc: 0.7616 - val_loss: 0.5403 - val_acc: 0.7573\n",
      "Epoch 4/10\n",
      "4300/4300 [==============================] - 7s 2ms/step - loss: 0.4574 - acc: 0.7984 - val_loss: 0.5067 - val_acc: 0.7742\n",
      "Epoch 5/10\n",
      "4300/4300 [==============================] - 6s 2ms/step - loss: 0.4260 - acc: 0.8119 - val_loss: 0.4784 - val_acc: 0.7757\n",
      "Epoch 6/10\n",
      "4300/4300 [==============================] - 7s 2ms/step - loss: 0.3904 - acc: 0.8258 - val_loss: 0.4622 - val_acc: 0.7834\n",
      "Epoch 7/10\n",
      "4300/4300 [==============================] - 7s 2ms/step - loss: 0.3495 - acc: 0.8440 - val_loss: 0.4364 - val_acc: 0.8003\n",
      "Epoch 8/10\n",
      "4300/4300 [==============================] - 7s 2ms/step - loss: 0.3109 - acc: 0.8721 - val_loss: 0.4969 - val_acc: 0.7788\n",
      "Epoch 9/10\n",
      "4300/4300 [==============================] - 7s 2ms/step - loss: 0.2829 - acc: 0.8786 - val_loss: 0.4519 - val_acc: 0.8049\n",
      "Epoch 10/10\n",
      "4300/4300 [==============================] - 7s 2ms/step - loss: 0.2500 - acc: 0.9009 - val_loss: 0.4589 - val_acc: 0.8065\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "patience = 20\n",
    "\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=patience)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = model.fit(sequences_matrix_train, Xy_train, \n",
    "                 validation_data=(sequences_matrix_test[:n_items_for_test//2], \n",
    "                                  Xy_test[:n_items_for_test//2]), \n",
    "                 epochs=epochs, batch_size=batch_size, shuffle=True,\n",
    "                 callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8064516186714172"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['val_acc'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8) \n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# # summarize history for loss\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(sequences_matrix_test[n_items_for_test//2:])\n",
    "pred.shape\n",
    "bin_pred, real = [], np.array(Xy_test)[n_items_for_test//2:]\n",
    "for i in range(len(pred)):\n",
    "    #print(f'pred = {int(round(float(pred[i][0]), 0))},\\t real = {real[i]}')\n",
    "    bin_pred.append(int(round(float(pred[i][0]), 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bin_pred), len(real), len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report([r[0] for r in real], bin_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(confusion_matrix([r[0] for r in real], bin_pred), columns=['Pos', 'Neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
